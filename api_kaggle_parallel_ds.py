import argparse
import os
import re
import sys

now_dir = os.getcwd()
sys.path.append(now_dir)
sys.path.append("%s/GPT_SoVITS" % (now_dir))

import signal
from text.LangSegmenter import LangSegmenter
from time import time as ttime
import torch
import torchaudio
import librosa
import soundfile as sf
from fastapi import FastAPI, Request, Query
from fastapi.responses import StreamingResponse, JSONResponse
import uvicorn
from transformers import AutoModelForMaskedLM, AutoTokenizer
import numpy as np
from feature_extractor import cnhubert
from io import BytesIO
from module.models import Generator, SynthesizerTrn, SynthesizerTrnV3
from peft import LoraConfig, get_peft_model
from AR.models.t2s_lightning_module import Text2SemanticLightningModule
from text import cleaned_text_to_sequence
from text.cleaner import clean_text
from module.mel_processing import spectrogram_torch
import config as global_config
import logging
import subprocess

import logging.config
import uvicorn

logging.config.dictConfig(uvicorn.config.LOGGING_CONFIG)
logger = logging.getLogger("uvicorn")

# æ¨¡å‹å®ä¾‹è®¿é—®è®°å½•ï¼Œç”¨äºLRUæ·˜æ±°
model_access_times = {}
# å½“å‰å·²åŠ è½½çš„æ¨¡å‹è®¡æ•°
loaded_models_count = 0
# é•¿æ–‡æœ¬é˜ˆå€¼
long_text_threshold = 70 

# --- ä»è¿™é‡Œå¼€å§‹ï¼ŒæŠ„çš„ gradio_tunneling çš„æ ¸å¿ƒä»£ç  ---
import atexit
import platform
import stat
import time
from pathlib import Path
from typing import List, Optional

import requests

VERSION = "0.2"
CURRENT_TUNNELS: List["Tunnel"] = []

machine = platform.machine()
if machine == "x86_64":
    machine = "amd64"

BINARY_REMOTE_NAME = f"frpc_{platform.system().lower()}_{machine.lower()}"
EXTENSION = ".exe" if os.name == "nt" else ""
BINARY_URL = f"https://cdn-media.huggingface.co/frpc-gradio-{VERSION}/{BINARY_REMOTE_NAME}{EXTENSION}"

BINARY_FILENAME = f"{BINARY_REMOTE_NAME}_v{VERSION}"
BINARY_FOLDER = Path(__file__).parent.absolute()
BINARY_PATH = f"{BINARY_FOLDER / BINARY_FILENAME}"

TUNNEL_TIMEOUT_SECONDS = 30
TUNNEL_ERROR_MESSAGE = (
    "Could not create share URL. "
    "Please check the appended log from frpc for more information:"
)

GRADIO_API_SERVER = "https://api.gradio.app/v2/tunnel-request"
GRADIO_SHARE_SERVER_ADDRESS = None


class Tunnel:
    def __init__(self, remote_host, remote_port, local_host, local_port, share_token):
        self.proc = None
        self.url = None
        self.remote_host = remote_host
        self.remote_port = remote_port
        self.local_host = local_host
        self.local_port = local_port
        self.share_token = share_token

    @staticmethod
    def download_binary():
        if not Path(BINARY_PATH).exists():
            resp = requests.get(BINARY_URL)

            if resp.status_code == 403:
                raise OSError(
                    f"Cannot set up a share link as this platform is incompatible. Please "
                    f"create a GitHub issue with information about your platform: {platform.uname()}"
                )

            resp.raise_for_status()

            # Save file data to local copy
            with open(BINARY_PATH, "wb") as file:
                file.write(resp.content)
            st = os.stat(BINARY_PATH)
            os.chmod(BINARY_PATH, st.st_mode | stat.S_IEXEC)

    def start_tunnel(self) -> str:
        self.download_binary()
        self.url = self._start_tunnel(BINARY_PATH)
        return self.url

    def kill(self):
        if self.proc is not None:
            print(f"Killing tunnel {self.local_host}:{self.local_port} <> {self.url}")
            self.proc.terminate()
            self.proc = None

    def _start_tunnel(self, binary: str) -> str:
        CURRENT_TUNNELS.append(self)
        command = [
            binary,
            "http",
            "-n",
            self.share_token,
            "-l",
            str(self.local_port),
            "-i",
            self.local_host,
            "--uc",
            "--sd",
            "random",
            "--ue",
            "--server_addr",
            f"{self.remote_host}:{self.remote_port}",
            "--disable_log_color",
        ]
        self.proc = subprocess.Popen(
            command, stdout=subprocess.PIPE, stderr=subprocess.PIPE
        )
        atexit.register(self.kill)
        return self._read_url_from_tunnel_stream()

    def _read_url_from_tunnel_stream(self) -> str:
        start_timestamp = time.time()

        log = []
        url = ""

        def _raise_tunnel_error():
            log_text = "\n".join(log)
            print(log_text, file=sys.stderr)
            raise ValueError(f"{TUNNEL_ERROR_MESSAGE}\n{log_text}")

        while url == "":
            # check for timeout and log
            if time.time() - start_timestamp >= TUNNEL_TIMEOUT_SECONDS:
                _raise_tunnel_error()

            assert self.proc is not None
            if self.proc.stdout is None:
                continue

            line = self.proc.stdout.readline()
            line = line.decode("utf-8")

            if line == "":
                continue

            log.append(line.strip())

            if "start proxy success" in line:
                result = re.search("start proxy success: (.+)\n", line)
                if result is None:
                    _raise_tunnel_error()
                else:
                    url = result.group(1)
            elif "login to server failed" in line:
                _raise_tunnel_error()

        return url


def setup_tunnel(
    local_host: str,
    local_port: int,
    share_token: str,
    share_server_address: Optional[str],
) -> str:
    share_server_address = (
        GRADIO_SHARE_SERVER_ADDRESS
        if share_server_address is None
        else share_server_address
    )
    if share_server_address is None:
        response = requests.get(GRADIO_API_SERVER)
        if not (response and response.status_code == 200):
            raise RuntimeError("Could not get share link from Gradio API Server.")
        payload = response.json()[0]
        remote_host, remote_port = payload["host"], int(payload["port"])
    else:
        remote_host, remote_port = share_server_address.split(":")
        remote_port = int(remote_port)
    try:
        tunnel = Tunnel(remote_host, remote_port, local_host, local_port, share_token)
        address = tunnel.start_tunnel()
        return address
    except Exception as e:
        raise RuntimeError(str(e)) from e
# --- ç»“æŸ ---

# ============ GPUç¯å¢ƒæ£€æµ‹ ============
def check_gpu_availability():
    """æ£€æµ‹å¯ç”¨çš„GPUæ•°é‡"""
    gpu_count = torch.cuda.device_count()
    logger.info(f"âœ… æ£€æµ‹åˆ° {gpu_count} ä¸ªGPUè®¾å¤‡")
    
    if gpu_count == 0:
        logger.warning("âŒ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
        return 0, ["cpu"]
    elif gpu_count == 1:
        logger.info("ğŸ”§ å•GPUç¯å¢ƒï¼Œå¯ç”¨å•å¡ä¼˜åŒ–æ¨¡å¼")
        return 1, ["cuda:0"]
    else:
        logger.info(f"ğŸš€ å¤šGPUç¯å¢ƒï¼Œå¯ç”¨å¹¶è¡Œæ¨¡å¼")
        return gpu_count, [f"cuda:{i}" for i in range(gpu_count)]

# æ£€æµ‹GPU
GPU_COUNT, GPU_LIST = check_gpu_availability()
IS_MULTI_GPU = GPU_COUNT > 1

class DefaultRefer:
    def __init__(self, path, text, language):
        self.path = args.default_refer_path
        self.text = args.default_refer_text
        self.language = args.default_refer_language

    def is_ready(self) -> bool:
        return is_full(self.path, self.text, self.language)


def is_empty(*items):  # ä»»æ„ä¸€é¡¹ä¸ä¸ºç©ºè¿”å›False
    for item in items:
        if item is not None and item != "":
            return False
    return True


def is_full(*items):  # ä»»æ„ä¸€é¡¹ä¸ºç©ºè¿”å›False
    for item in items:
        if item is None or item == "":
            return False
    return True


bigvgan_model = hifigan_model = sv_cn_model = None
def clean_hifigan_model():
    global hifigan_model
    if hifigan_model:
        hifigan_model = hifigan_model.cpu()
        hifigan_model = None
        try:
            torch.cuda.empty_cache()
        except:
            pass
def clean_bigvgan_model():
    global bigvgan_model
    if bigvgan_model:
        bigvgan_model = bigvgan_model.cpu()
        bigvgan_model = None
        try:
            torch.cuda.empty_cache()
        except:
            pass
def clean_sv_cn_model():
    global sv_cn_model
    if sv_cn_model:
        sv_cn_model.embedding_model = sv_cn_model.embedding_model.cpu()
        sv_cn_model = None
        try:
            torch.cuda.empty_cache()
        except:
            pass


def init_bigvgan(target_gpu="cuda:0"):
    # å£°æ˜å…¨å±€å˜é‡
    global bigvgan_model
    
    from BigVGAN import bigvgan
    
    bigvgan_model = bigvgan.BigVGAN.from_pretrained(
        "%s/GPT_SoVITS/pretrained_models/models--nvidia--bigvgan_v2_24khz_100band_256x" % (now_dir,),
        use_cuda_kernel=False,
    )
    bigvgan_model.remove_weight_norm()
    bigvgan_model = bigvgan_model.eval()
    
    if is_half == True:
        bigvgan_model = bigvgan_model.half().to(target_gpu)
    else:
        bigvgan_model = bigvgan_model.to(target_gpu)

def init_hifigan(target_gpu="cuda:0"):
    # å£°æ˜å…¨å±€å˜é‡
    global hifigan_model
    
    hifigan_model = Generator(
        initial_channel=100,
        resblock="1",
        resblock_kernel_sizes=[3, 7, 11],
        resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],
        upsample_rates=[10, 6, 2, 2, 2],
        upsample_initial_channel=512,
        upsample_kernel_sizes=[20, 12, 4, 4, 4],
        gin_channels=0,
        is_bias=True,
    )
    hifigan_model.eval()
    hifigan_model.remove_weight_norm()
    state_dict_g = torch.load(
        "%s/GPT_SoVITS/pretrained_models/gsv-v4-pretrained/vocoder.pth" % (now_dir,),
        map_location="cpu",
        weights_only=False,
    )
    print("loading vocoder", hifigan_model.load_state_dict(state_dict_g))
    if is_half == True:
        hifigan_model = hifigan_model.half().to(target_gpu)
    else:
        hifigan_model = hifigan_model.to(target_gpu)

from sv import SV
def init_sv_cn():
    global hifigan_model
    sv_cn_model = SV(device, is_half)


resample_transform_dict = {}


def resample(audio_tensor, sr0, sr1, device):
    global resample_transform_dict
    key = "%s-%s-%s" % (sr0, sr1, str(device))
    if key not in resample_transform_dict:
        resample_transform_dict[key] = torchaudio.transforms.Resample(sr0, sr1).to(device)
    return resample_transform_dict[key](audio_tensor)


from module.mel_processing import mel_spectrogram_torch

spec_min = -12
spec_max = 2


def norm_spec(x):
    return (x - spec_min) / (spec_max - spec_min) * 2 - 1


def denorm_spec(x):
    return (x + 1) / 2 * (spec_max - spec_min) + spec_min


mel_fn = lambda x: mel_spectrogram_torch(
    x,
    **{
        "n_fft": 1024,
        "win_size": 1024,
        "hop_size": 256,
        "num_mels": 100,
        "sampling_rate": 24000,
        "fmin": 0,
        "fmax": None,
        "center": False,
    },
)
mel_fn_v4 = lambda x: mel_spectrogram_torch(
    x,
    **{
        "n_fft": 1280,
        "win_size": 1280,
        "hop_size": 320,
        "num_mels": 100,
        "sampling_rate": 32000,
        "fmin": 0,
        "fmax": None,
        "center": False,
    },
)


sr_model = None


def audio_sr(audio, sr):
    global sr_model
    if sr_model == None:
        from tools.audio_sr import AP_BWE

        try:
            sr_model = AP_BWE(device, DictToAttrRecursive)
        except FileNotFoundError:
            logger.info("ä½ æ²¡æœ‰ä¸‹è½½è¶…åˆ†æ¨¡å‹çš„å‚æ•°ï¼Œå› æ­¤ä¸è¿›è¡Œè¶…åˆ†ã€‚å¦‚æƒ³è¶…åˆ†è¯·å…ˆå‚ç…§æ•™ç¨‹æŠŠæ–‡ä»¶ä¸‹è½½")
            return audio.cpu().detach().numpy(), sr
    return sr_model(audio, sr)


# 2. ä¿®æ”¹ Speaker ç±»ï¼Œæ·»åŠ  gpt_pathã€sovits_path
class Speaker:
    def __init__(self, name, gpt=None, sovits=None, phones=None, bert=None, prompt=None, gpt_path=None, sovits_path=None, load_time=None,gpu0_gpt=None, gpu0_sovits=None, gpu1_gpt=None, gpu1_sovits=None,
last_used=None):
        self.name = name
        self.gpt = gpt
        self.sovits = sovits
        self.phones = phones
        self.bert = bert
        self.prompt = prompt
        self.gpt_path = gpt_path
        self.sovits_path = sovits_path
        # åŒGPUæ‰©å±•å­—æ®µ
        self.gpu0_gpt = gpu0_gpt
        self.gpu0_sovits = gpu0_sovits
        self.gpu1_gpt = gpu1_gpt
        self.gpu1_sovits = gpu1_sovits
        self.last_used = last_used

class Sovits:
    def __init__(self, vq_model, hps):
        self.vq_model = vq_model
        self.hps = hps


from process_ckpt import get_sovits_version_from_path_fast, load_sovits_new


def get_sovits_weights(sovits_path, target_gpu="cuda:0"):
    from config import pretrained_sovits_name
    path_sovits_v3 = pretrained_sovits_name["v3"]
    path_sovits_v4 = pretrained_sovits_name["v4"]
    is_exist_s2gv3 = os.path.exists(path_sovits_v3)
    is_exist_s2gv4 = os.path.exists(path_sovits_v4)

    version, model_version, if_lora_v3 = get_sovits_version_from_path_fast(sovits_path)
    is_exist = is_exist_s2gv3 if model_version == "v3" else is_exist_s2gv4
    path_sovits = path_sovits_v3 if model_version == "v3" else path_sovits_v4

    if if_lora_v3 == True and is_exist == False:
        logger.info("SoVITS %s åº•æ¨¡ç¼ºå¤±ï¼Œæ— æ³•åŠ è½½ç›¸åº” LoRA æƒé‡" % model_version)

    dict_s2 = load_sovits_new(sovits_path)
    hps = dict_s2["config"]
    hps = DictToAttrRecursive(hps)
    hps.model.semantic_frame_rate = "25hz"
    if "enc_p.text_embedding.weight" not in dict_s2["weight"]:
        hps.model.version = "v2"  # v3model,v2sybomls
    elif dict_s2["weight"]["enc_p.text_embedding.weight"].shape[0] == 322:
        hps.model.version = "v1"
    else:
        hps.model.version = "v2"

    model_params_dict = vars(hps.model)
    if model_version not in {"v3", "v4"}:
        if "Pro" in model_version:
            hps.model.version = model_version
            if sv_cn_model == None:
                init_sv_cn()

        vq_model = SynthesizerTrn(
            hps.data.filter_length // 2 + 1,
            hps.train.segment_size // hps.data.hop_length,
            n_speakers=hps.data.n_speakers,
            **model_params_dict,
        )
    else:
        hps.model.version = model_version
        vq_model = SynthesizerTrnV3(
            hps.data.filter_length // 2 + 1,
            hps.train.segment_size // hps.data.hop_length,
            n_speakers=hps.data.n_speakers,
            **model_params_dict,
        )
        if model_version == "v3":
            init_bigvgan()
        if model_version == "v4":
            init_hifigan()

    model_version = hps.model.version
    logger.info(f"æ¨¡å‹ç‰ˆæœ¬: {model_version}")
    if "pretrained" not in sovits_path:
        try:
            del vq_model.enc_q
        except:
            pass
    if is_half == True:
        vq_model = vq_model.half().to(target_gpu)
    else:
        vq_model = vq_model.to(target_gpu)
    vq_model.eval()
    if if_lora_v3 == False:
        vq_model.load_state_dict(dict_s2["weight"], strict=False)
    else:
        path_sovits = path_sovits_v3 if model_version == "v3" else path_sovits_v4
        vq_model.load_state_dict(load_sovits_new(path_sovits)["weight"], strict=False)
        lora_rank = dict_s2["lora_rank"]
        lora_config = LoraConfig(
            target_modules=["to_k", "to_q", "to_v", "to_out.0"],
            r=lora_rank,
            lora_alpha=lora_rank,
            init_lora_weights=True,
        )
        vq_model.cfm = get_peft_model(vq_model.cfm, lora_config)
        vq_model.load_state_dict(dict_s2["weight"], strict=False)
        vq_model.cfm = vq_model.cfm.merge_and_unload()
        # torch.save(vq_model.state_dict(),"merge_win.pth")
        vq_model.eval()

    sovits = Sovits(vq_model, hps)
    return sovits


class Gpt:
    def __init__(self, max_sec, t2s_model):
        self.max_sec = max_sec
        self.t2s_model = t2s_model


global hz
hz = 50


def get_gpt_weights(gpt_path, target_gpu="cuda:0"):
    dict_s1 = torch.load(gpt_path, map_location="cpu", weights_only=False)
    config = dict_s1["config"]
    max_sec = config["data"]["max_sec"]
    t2s_model = _safe_model_load(
     lambda: Text2SemanticLightningModule(config, "****", is_train=False)
    )
    t2s_model.load_state_dict(dict_s1["weight"])
    if is_half == True:
        t2s_model = t2s_model.half()
    t2s_model = t2s_model.to(target_gpu)
    t2s_model.eval()
    # total = sum([param.nelement() for param in t2s_model.parameters()])
    # logger.info("Number of parameter: %.2fM" % (total / 1e6))

    gpt = Gpt(max_sec, t2s_model)
    return gpt


# 8. ä¿®æ”¹ change_gpt_sovits_weights
def change_gpt_sovits_weights(gpt_path, sovits_path, speaker_id="default"):
    try:
        speaker_list[speaker_id] = Speaker(
            name=speaker_id,
            gpt=None,
            sovits=None,
            prompt=speaker_list.get(speaker_id, Speaker(name=speaker_id, gpt=None, sovits=None)).prompt or {
                "path": "D.wav",
                "text": "æ­Œæ‰‹ã§ã‚¤ãƒ—ã‚·ãƒ­ãƒ³ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚¹ã‚¿ãƒ¼â€¦ã€å‚·ã¤ãèª°ã‹ã®å¿ƒã‚’å®ˆã‚‹ã“ã¨ãŒã§ããŸãªã‚‰ã€ã£ã¦ã€ã‚¢ãƒŠã‚¿ã®ä½œå“ã ã‚ˆã­ï¼Ÿ",
                "prompt_language": "ja"
            },
            gpt_path=gpt_path,
            sovits_path=sovits_path
        )
        return JSONResponse({"code": 0, "message": "Success"}, status_code=200)
    except Exception as e:
        return JSONResponse({"code": 400, "message": str(e)}, status_code=400)


def get_bert_feature(text, word2ph):
    with torch.no_grad():
        inputs = tokenizer(text, return_tensors="pt")
        for i in inputs:
            inputs[i] = inputs[i].to(device)  #####è¾“å…¥æ˜¯longä¸ç”¨ç®¡ç²¾åº¦é—®é¢˜ï¼Œç²¾åº¦éšbert_model
        res = bert_model(**inputs, output_hidden_states=True)
        res = torch.cat(res["hidden_states"][-3:-2], -1)[0].cpu()[1:-1]
    assert len(word2ph) == len(text)
    phone_level_feature = []
    for i in range(len(word2ph)):
        repeat_feature = res[i].repeat(word2ph[i], 1)
        phone_level_feature.append(repeat_feature)
    phone_level_feature = torch.cat(phone_level_feature, dim=0)
    # if(is_half==True):phone_level_feature=phone_level_feature.half()
    return phone_level_feature.T


def clean_text_inf(text, language, version):
    language = language.replace("all_", "")
    phones, word2ph, norm_text = clean_text(text, language, version)
    phones = cleaned_text_to_sequence(phones, version)
    return phones, word2ph, norm_text


def get_bert_inf(phones, word2ph, norm_text, language):
    language = language.replace("all_", "")
    if language == "zh":
        bert = get_bert_feature(norm_text, word2ph).to(device)  # .to(dtype)
    else:
        bert = torch.zeros(
            (1024, len(phones)),
            dtype=torch.float16 if is_half == True else torch.float32,
        ).to(device)

    return bert


from text import chinese


def get_phones_and_bert(text, language, version, final=False):
    text = re.sub(r' {2,}', ' ', text)
    textlist = []
    langlist = []
    if language == "all_zh":
        for tmp in LangSegmenter.getTexts(text,"zh"):
            langlist.append(tmp["lang"])
            textlist.append(tmp["text"])
    elif language == "all_yue":
        for tmp in LangSegmenter.getTexts(text,"zh"):
            if tmp["lang"] == "zh":
                tmp["lang"] = "yue"
            langlist.append(tmp["lang"])
            textlist.append(tmp["text"])
    elif language == "all_ja":
        for tmp in LangSegmenter.getTexts(text,"ja"):
            langlist.append(tmp["lang"])
            textlist.append(tmp["text"])
    elif language == "all_ko":
        for tmp in LangSegmenter.getTexts(text,"ko"):
            langlist.append(tmp["lang"])
            textlist.append(tmp["text"])
    elif language == "en":
        langlist.append("en")
        textlist.append(text)
    elif language == "auto":
        for tmp in LangSegmenter.getTexts(text):
            langlist.append(tmp["lang"])
            textlist.append(tmp["text"])
    elif language == "auto_yue":
        for tmp in LangSegmenter.getTexts(text):
            if tmp["lang"] == "zh":
                tmp["lang"] = "yue"
            langlist.append(tmp["lang"])
            textlist.append(tmp["text"])
    else:
        for tmp in LangSegmenter.getTexts(text):
            if langlist:
                if (tmp["lang"] == "en" and langlist[-1] == "en") or (tmp["lang"] != "en" and langlist[-1] != "en"):
                    textlist[-1] += tmp["text"]
                    continue
            if tmp["lang"] == "en":
                langlist.append(tmp["lang"])
            else:
                # å› æ— æ³•åŒºåˆ«ä¸­æ—¥éŸ©æ–‡æ±‰å­—,ä»¥ç”¨æˆ·è¾“å…¥ä¸ºå‡†
                langlist.append(language)
            textlist.append(tmp["text"])
    phones_list = []
    bert_list = []
    norm_text_list = []
    for i in range(len(textlist)):
        lang = langlist[i]
        phones, word2ph, norm_text = clean_text_inf(textlist[i], lang, version)
        bert = get_bert_inf(phones, word2ph, norm_text, lang)
        phones_list.append(phones)
        norm_text_list.append(norm_text)
        bert_list.append(bert)
    bert = torch.cat(bert_list, dim=1)
    phones = sum(phones_list, [])
    norm_text = "".join(norm_text_list)

    if not final and len(phones) < 6:
        return get_phones_and_bert("." + text, language, version, final=True)

    return phones, bert.to(torch.float16 if is_half == True else torch.float32), norm_text


class DictToAttrRecursive(dict):
    def __init__(self, input_dict):
        super().__init__(input_dict)
        for key, value in input_dict.items():
            if isinstance(value, dict):
                value = DictToAttrRecursive(value)
            self[key] = value
            setattr(self, key, value)

    def __getattr__(self, item):
        try:
            return self[item]
        except KeyError:
            raise AttributeError(f"Attribute {item} not found")

    def __setattr__(self, key, value):
        if isinstance(value, dict):
            value = DictToAttrRecursive(value)
        super(DictToAttrRecursive, self).__setitem__(key, value)
        super().__setattr__(key, value)

    def __delattr__(self, item):
        try:
            del self[item]
        except KeyError:
            raise AttributeError(f"Attribute {item} not found")


def get_spepc(hps, filename, dtype, device, is_v2pro=False):
    sr1 = int(hps.data.sampling_rate)
    audio, sr0 = torchaudio.load(filename)
    if sr0 != sr1:
        audio = audio.to(device)
        if audio.shape[0] == 2:
            audio = audio.mean(0).unsqueeze(0)
        audio = resample(audio, sr0, sr1, device)
    else:
        audio = audio.to(device)
        if audio.shape[0] == 2:
            audio = audio.mean(0).unsqueeze(0)

    maxx = audio.abs().max()
    if maxx > 1:
        audio /= min(2, maxx)
    spec = spectrogram_torch(
        audio,
        hps.data.filter_length,
        hps.data.sampling_rate,
        hps.data.hop_length,
        hps.data.win_length,
        center=False,
    )
    spec = spec.to(dtype)
    if is_v2pro == True:
        audio = resample(audio, sr1, 16000, device).to(dtype)
    return spec, audio


def pack_audio(audio_bytes, data, rate):
    if media_type == "ogg":
        audio_bytes = pack_ogg(audio_bytes, data, rate)
    elif media_type == "aac":
        audio_bytes = pack_aac(audio_bytes, data, rate)
    else:
        # wavæ— æ³•æµå¼, å…ˆæš‚å­˜raw
        audio_bytes = pack_raw(audio_bytes, data, rate)

    return audio_bytes


def pack_ogg(audio_bytes, data, rate):
    # Author: AkagawaTsurunaki
    # Issue:
    #   Stack overflow probabilistically occurs
    #   when the function `sf_writef_short` of `libsndfile_64bit.dll` is called
    #   using the Python library `soundfile`
    # Note:
    #   This is an issue related to `libsndfile`, not this project itself.
    #   It happens when you generate a large audio tensor (about 499804 frames in my PC)
    #   and try to convert it to an ogg file.
    # Related:
    #   https://github.com/RVC-Boss/GPT-SoVITS/issues/1199
    #   https://github.com/libsndfile/libsndfile/issues/1023
    #   https://github.com/bastibe/python-soundfile/issues/396
    # Suggestion:
    #   Or split the whole audio data into smaller audio segment to avoid stack overflow?

    def handle_pack_ogg():
        with sf.SoundFile(audio_bytes, mode="w", samplerate=rate, channels=1, format="ogg") as audio_file:
            audio_file.write(data)

    import threading

    # See: https://docs.python.org/3/library/threading.html
    # The stack size of this thread is at least 32768
    # If stack overflow error still occurs, just modify the `stack_size`.
    # stack_size = n * 4096, where n should be a positive integer.
    # Here we chose n = 4096.
    stack_size = 4096 * 4096
    try:
        threading.stack_size(stack_size)
        pack_ogg_thread = threading.Thread(target=handle_pack_ogg)
        pack_ogg_thread.start()
        pack_ogg_thread.join()
    except RuntimeError as e:
        # If changing the thread stack size is unsupported, a RuntimeError is raised.
        print("RuntimeError: {}".format(e))
        print("Changing the thread stack size is unsupported.")
    except ValueError as e:
        # If the specified stack size is invalid, a ValueError is raised and the stack size is unmodified.
        print("ValueError: {}".format(e))
        print("The specified stack size is invalid.")

    return audio_bytes


def pack_raw(audio_bytes, data, rate):
    audio_bytes.write(data.tobytes())

    return audio_bytes


def pack_wav(audio_bytes, rate):
    if is_int32:
        data = np.frombuffer(audio_bytes.getvalue(), dtype=np.int32)
        wav_bytes = BytesIO()
        sf.write(wav_bytes, data, rate, format="WAV", subtype="PCM_32")
    else:
        data = np.frombuffer(audio_bytes.getvalue(), dtype=np.int16)
        wav_bytes = BytesIO()
        sf.write(wav_bytes, data, rate, format="WAV")
    return wav_bytes


def pack_aac(audio_bytes, data, rate):
    if is_int32:
        pcm = "s32le"
        bit_rate = "256k"
    else:
        pcm = "s16le"
        bit_rate = "128k"
    process = subprocess.Popen(
        [
            "ffmpeg",
            "-f",
            pcm,  # è¾“å…¥16ä½æœ‰ç¬¦å·å°ç«¯æ•´æ•°PCM
            "-ar",
            str(rate),  # è®¾ç½®é‡‡æ ·ç‡
            "-ac",
            "1",  # å•å£°é“
            "-i",
            "pipe:0",  # ä»ç®¡é“è¯»å–è¾“å…¥
            "-c:a",
            "aac",  # éŸ³é¢‘ç¼–ç å™¨ä¸ºAAC
            "-b:a",
            bit_rate,  # æ¯”ç‰¹ç‡
            "-vn",  # ä¸åŒ…å«è§†é¢‘
            "-f",
            "adts",  # è¾“å‡ºAACæ•°æ®æµæ ¼å¼
            "pipe:1",  # å°†è¾“å‡ºå†™å…¥ç®¡é“
        ],
        stdin=subprocess.PIPE,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
    )
    out, _ = process.communicate(input=data.tobytes())
    audio_bytes.write(out)

    return audio_bytes


def read_clean_buffer(audio_bytes):
    audio_chunk = audio_bytes.getvalue()
    audio_bytes.truncate(0)
    audio_bytes.seek(0)

    return audio_bytes, audio_chunk


def cut_text(text, punc):
    punc_list = [p for p in punc if p in {",", ".", ";", "?", "!", "ã€", "ï¼Œ", "ã€‚", "ï¼Ÿ", "ï¼", "ï¼›", "ï¼š", "â€¦"}]
    if len(punc_list) > 0:
        punds = r"[" + "".join(punc_list) + r"]"
        text = text.strip("\n")
        items = re.split(f"({punds})", text)
        mergeitems = ["".join(group) for group in zip(items[::2], items[1::2])]
        # åœ¨å¥å­ä¸å­˜åœ¨ç¬¦å·æˆ–å¥å°¾æ— ç¬¦å·çš„æ—¶å€™ä¿è¯æ–‡æœ¬å®Œæ•´
        if len(items) % 2 == 1:
            mergeitems.append(items[-1])
        text = "\n".join(mergeitems)

    while "\n\n" in text:
        text = text.replace("\n\n", "\n")

    return text


def only_punc(text):
    return not any(t.isalnum() or t.isalpha() for t in text)


splits = {
    "ï¼Œ",
    "ã€‚",
    "ï¼Ÿ",
    "ï¼",
    ",",
    ".",
    "?",
    "!",
    "~",
    ":",
    "ï¼š",
    "â€”",
    "â€¦",
}


def unload_least_recently_used():
    """å¸è½½æœ€ä¹…æœªä½¿ç”¨çš„æ¨¡å‹ï¼ˆåŒæ—¶å¸è½½è¯¥è¯´è¯äººåœ¨ä¸¤ä¸ªGPUä¸Šçš„å®ä¾‹ï¼‰"""
    # å£°æ˜å…¨å±€å˜é‡
    global loaded_models_count
    
    # æ‰¾åˆ°æœ€ä¹…æœªä½¿ç”¨çš„è¯´è¯äºº
    if not speaker_list:
        return
    
    # æ‰¾åˆ°æœ‰æ¨¡å‹åŠ è½½ä¸”æœ€ä¹…æœªä½¿ç”¨çš„è¯´è¯äºº
    candidates = []
    for speaker_id, speaker in speaker_list.items():
        if speaker.gpu0_gpt is not None or speaker.gpu1_gpt is not None:
            candidates.append((speaker_id, speaker.last_used or 0))
    
    if not candidates:
        return
    
    oldest_speaker_id = min(candidates, key=lambda x: x[1])[0]
    oldest_speaker = speaker_list[oldest_speaker_id]
    
    # åŒæ—¶å¸è½½è¯¥è¯´è¯äººåœ¨ä¸¤ä¸ªGPUä¸Šçš„å®ä¾‹
    if oldest_speaker.gpu0_gpt is not None:
        oldest_speaker.gpu0_gpt = None
        oldest_speaker.gpu0_sovits = None
        loaded_models_count -= 1
    
    if oldest_speaker.gpu1_gpt is not None:
        oldest_speaker.gpu1_gpt = None
        oldest_speaker.gpu1_sovits = None
        loaded_models_count -= 1
    
    # æ¸…ç†GPUç¼“å­˜
    torch.cuda.empty_cache()
    
    logger.info(f"ğŸ”„ å·²å¸è½½æœ€ä¹…æœªä½¿ç”¨çš„è¯´è¯äºº: {oldest_speaker_id}")

def ensure_model_loaded(speaker_id, gpu_index=None):
    """
    ç¡®ä¿æŒ‡å®šè¯´è¯äººçš„æ¨¡å‹åŠ è½½åˆ°æŒ‡å®šçš„GPUä¸Š
    è‡ªåŠ¨é€‚é…å•GPUç¯å¢ƒ
    """
    from time import time as ttime
    
    # å¿…é¡»åœ¨å‡½æ•°é¡¶éƒ¨å£°æ˜æ‰€æœ‰è¦ä¿®æ”¹çš„å…¨å±€å˜é‡
    global loaded_models_count, model_access_times
    
    if speaker_id not in speaker_list:
        raise ValueError(f"Speaker {speaker_id} not found")
    
    speaker = speaker_list[speaker_id]
    speaker.last_used = ttime()
    
    # æ›´æ–°å…¨å±€è®¿é—®è®°å½•ï¼ˆç”¨äºLRUæ·˜æ±°ï¼‰
    model_access_times[speaker_id] = ttime()

    # å•GPUç¯å¢ƒï¼šåªä½¿ç”¨GPU 0
    if not IS_MULTI_GPU:
        gpu_index = 0
        target_gpu = "cuda:0"
        
        if speaker.gpu0_gpt is None:
            if loaded_models_count >= max_models:
                unload_least_recently_used()
            
            speaker.gpu0_gpt = get_gpt_weights(speaker.gpt_path, target_gpu)
            speaker.gpu0_sovits = get_sovits_weights(speaker.sovits_path, target_gpu)
            loaded_models_count += 1
        return
    
    # å¦‚æœæŒ‡å®šäº†GPUç´¢å¼•
    if gpu_index is not None:
        # æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨
        if gpu_index >= torch.cuda.device_count():
            raise ValueError(f"GPU {gpu_index} ä¸å¯ç”¨ï¼Œå¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}")
        
        target_gpu = f"cuda:{gpu_index}"
        
        # æ£€æŸ¥è¯¥GPUä¸Šçš„æ¨¡å‹æ˜¯å¦å·²åŠ è½½
        if gpu_index == 0:
            if speaker.gpu0_gpt is None:
                # æ£€æŸ¥å†…å­˜æ˜¯å¦å……è¶³
                free_mem, total_mem = check_gpu_memory(gpu_index)
                if free_mem < 1.0:  # å°‘äº1GBç©ºé—²å†…å­˜
                    logger.warning(f"GPU{gpu_index} å†…å­˜ä¸è¶³ ({free_mem:.2f}GB)ï¼Œå°è¯•æ¸…ç†...")
                    torch.cuda.empty_cache()
                    free_mem, _ = check_gpu_memory(gpu_index)
                    if free_mem < 0.5:  # æ¸…ç†åä»å°‘äº500MB
                        # å¸è½½æœ€ä¹…æœªä½¿ç”¨çš„æ¨¡å‹
                        unload_least_recently_used()
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦å¸è½½æ—§æ¨¡å‹
                if loaded_models_count >= max_models * 2:
                    unload_least_recently_used()
                
                # åŠ è½½æ¨¡å‹
                logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹åˆ° GPU{gpu_index}...")
                speaker.gpu0_gpt = get_gpt_weights(speaker.gpt_path, target_gpu)
                speaker.gpu0_sovits = get_sovits_weights(speaker.sovits_path, target_gpu)
                loaded_models_count += 1
                logger.info(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ° GPU{gpu_index}")
        
        elif gpu_index == 1:
            if speaker.gpu1_gpt is None:
                # æ£€æŸ¥å†…å­˜æ˜¯å¦å……è¶³
                free_mem, total_mem = check_gpu_memory(gpu_index)
                if free_mem < 1.0:  # å°‘äº1GBç©ºé—²å†…å­˜
                    logger.warning(f"GPU{gpu_index} å†…å­˜ä¸è¶³ ({free_mem:.2f}GB)ï¼Œå°è¯•æ¸…ç†...")
                    torch.cuda.empty_cache()
                    free_mem, _ = check_gpu_memory(gpu_index)
                    if free_mem < 0.5:  # æ¸…ç†åä»å°‘äº500MB
                        # å¸è½½æœ€ä¹…æœªä½¿ç”¨çš„æ¨¡å‹
                        unload_least_recently_used()
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦å¸è½½æ—§æ¨¡å‹
                if loaded_models_count >= max_models * 2:
                    unload_least_recently_used()
                
                # åŠ è½½æ¨¡å‹
                logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹åˆ° GPU{gpu_index}...")
                speaker.gpu1_gpt = get_gpt_weights(speaker.gpt_path, target_gpu)
                speaker.gpu1_sovits = get_sovits_weights(speaker.sovits_path, target_gpu)
                loaded_models_count += 1
                logger.info(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ° GPU{gpu_index}")
    
    # å¦‚æœæœªæŒ‡å®šGPUï¼Œç¡®ä¿è‡³å°‘ä¸€ä¸ªGPUæœ‰æ¨¡å‹
    else:
        if speaker.gpu0_gpt is None and speaker.gpu1_gpt is None:
            # é€‰æ‹©å†…å­˜æ›´å……è¶³çš„GPU
            free_mem_0, _ = check_gpu_memory(0)
            free_mem_1, _ = check_gpu_memory(1)
            
            selected_gpu = 0 if free_mem_0 >= free_mem_1 else 1
            logger.info(f"è‡ªåŠ¨é€‰æ‹© GPU{selected_gpu} (ç©ºé—²å†…å­˜: {max(free_mem_0, free_mem_1):.2f}GB)")
            
            ensure_model_loaded(speaker_id, selected_gpu)

# ä¿®æ”¹ get_tts_wav å‡½æ•°ï¼Œæ­£ç¡®å¤„ç† get_spepc çš„è¿”å›å€¼
def get_tts_wav(
    refer_wav_path,
    prompt_text,
    prompt_language,
    text,
    text_language,
    top_k=15,
    top_p=0.6,
    temperature=0.6,
    speed=1.0,
    inp_refs=None,
    sample_steps=32,
    if_sr=False,
    spk="default"
):
    from time import time as ttime
    import asyncio
    import concurrent.futures
    import numpy as np

    # æ ¹æ®GPUç¯å¢ƒå†³å®šæ˜¯å¦å¯ç”¨å¹¶è¡Œ
    if IS_MULTI_GPU:
        is_long, text_segments = split_long_text(text, long_text_threshold)
    else:
        # å•GPUç¯å¢ƒï¼šé•¿æ–‡æœ¬ä¹Ÿä½¿ç”¨å•å¡å¤„ç†
        is_long = False
        text_segments = [text]
        logger.info("ğŸ”§ å•GPUç¯å¢ƒï¼Œç¦ç”¨å¹¶è¡Œå¤„ç†")
    
    # æ›´æ–°æ¨¡å‹è®¿é—®æ—¶é—´
    if spk in speaker_list:
        speaker_list[spk].last_used = ttime()
    
    # 2. è·å–è¯´è¯äººä¿¡æ¯
    if spk not in speaker_list:
        raise ValueError(f"Speaker {spk} not found")
    
    speaker = speaker_list[spk]
    
    # 3. æ ¹æ®æ˜¯å¦é•¿æ–‡æœ¬é€‰æ‹©å¤„ç†æ–¹å¼
    if is_long:
        # ============ é•¿æ–‡æœ¬å¹¶è¡Œå¤„ç† ============
        logger.info(f"ğŸ“– é•¿æ–‡æœ¬æ£€æµ‹ ({len(text)}å­— > {long_text_threshold})ï¼Œå¯ç”¨åŒGPUå¹¶è¡Œå¤„ç†")
        
        # 3.1 ç¡®ä¿ä¸¤ä¸ªGPUä¸Šéƒ½æœ‰æ¨¡å‹
        ensure_model_loaded(spk, 0)
        ensure_model_loaded(spk, 1)
        
        # 3.2 å¹¶è¡Œå¤„ç†ä¸¤ä¸ªæ–‡æœ¬ç‰‡æ®µ
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            # å‡†å¤‡ä¸¤ä¸ªä»»åŠ¡
            futures = []
            for i, segment in enumerate(text_segments):
                if not segment.strip():  # è·³è¿‡ç©ºç‰‡æ®µ
                    continue
                    
                target_gpu = i % 2  # 0æˆ–1
                logger.info(f"  GPU{target_gpu} å¤„ç†ç‰‡æ®µ {i+1}: {segment[:30]}...")
                
                # æäº¤ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
                future = executor.submit(
                    _process_single_segment,
                    text_segment=segment,
                    refer_wav_path=refer_wav_path,
                    prompt_text=prompt_text,
                    prompt_language=prompt_language,
                    text_language=text_language,
                    top_k=top_k,
                    top_p=top_p,
                    temperature=temperature,
                    speed=speed,
                    inp_refs=inp_refs,
                    sample_steps=sample_steps,
                    if_sr=if_sr,
                    spk=spk,
                    gpu_index=target_gpu
                )
                futures.append((i, future))
            
            # 3.3 æ”¶é›†ç»“æœå¹¶ä¿æŒé¡ºåº
            segment_results = []
            for seg_idx, future in sorted(futures, key=lambda x: x[0]):
                try:
                    # è·å–éŸ³é¢‘æ•°æ®ï¼šaudio_array, sample_rate
                    audio_data, sr = future.result(timeout=120)  # 120ç§’è¶…æ—¶
                    segment_results.append((seg_idx, audio_data, sr))
                except concurrent.futures.TimeoutError:
                    logger.error(f"âŒ ç‰‡æ®µ {seg_idx} å¤„ç†è¶…æ—¶")
                    raise
                except Exception as e:
                    logger.error(f"âŒ ç‰‡æ®µ {seg_idx} å¤„ç†å¤±è´¥: {e}")
                    raise
        
        # 3.4 åˆå¹¶éŸ³é¢‘ç‰‡æ®µ
        if not segment_results:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„éŸ³é¢‘ç‰‡æ®µç”Ÿæˆ")
            
        # æŒ‰åŸå§‹é¡ºåºæ’åº
        segment_results.sort(key=lambda x: x[0])
        
        # è·å–ç¬¬ä¸€ä¸ªç‰‡æ®µçš„é‡‡æ ·ç‡ä½œä¸ºå‚è€ƒ
        first_audio, first_sr = segment_results[0][1], segment_results[0][2]
        
        # åˆå¹¶æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ
        all_audio_segments = []
        for seg_idx, (_, audio_data, sr) in enumerate(segment_results):
            # ç¡®ä¿é‡‡æ ·ç‡ä¸€è‡´
            if sr != first_sr:
                audio_data = _resample_audio_if_needed(audio_data, sr, first_sr)
            
            all_audio_segments.append(audio_data)
            
            # åœ¨ç‰‡æ®µé—´æ·»åŠ é™éŸ³ï¼ˆé™¤æœ€åä¸€ä¸ªç‰‡æ®µå¤–ï¼‰
            if seg_idx < len(segment_results) - 1:
                # åˆ›å»º0.3ç§’çš„é™éŸ³
                silence = np.zeros(int(first_sr * 0.3), dtype=audio_data.dtype)
                all_audio_segments.append(silence)
        
        # æ‹¼æ¥æ‰€æœ‰éŸ³é¢‘
        combined_audio = np.concatenate(all_audio_segments, axis=0)
        
        # æœ€ç»ˆå½’ä¸€åŒ–
        max_audio = np.abs(combined_audio).max()
        if max_audio > 1.0:
            combined_audio = combined_audio / max_audio
        
        final_audio = combined_audio
        final_sr = first_sr
        
    else:
        # ============ çŸ­æ–‡æœ¬å•GPUå¤„ç† ============
        logger.info(f"ğŸ“ çŸ­æ–‡æœ¬æ£€æµ‹ ({len(text)}å­— â‰¤ {long_text_threshold})ï¼Œä½¿ç”¨å•GPUå¤„ç†")
        
        # ç¡®å®šä½¿ç”¨å“ªä¸ªGPU
        selected_gpu = 0
        # æ£€æŸ¥å“ªä¸ªGPUæœ‰æ¨¡å‹
        if speaker.gpu1_gpt is not None and speaker.gpu0_gpt is None:
            selected_gpu = 1
        elif speaker.gpu0_gpt is not None:
            selected_gpu = 0
        else:
            # ä¸¤ä¸ªéƒ½æ²¡æœ‰ï¼Œéšæœºé€‰ä¸€ä¸ª
            import random
            selected_gpu = random.choice([0, 1])
        
        # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
        ensure_model_loaded(spk, selected_gpu)
        logger.info(f"  ä½¿ç”¨ GPU{selected_gpu} å¤„ç†çŸ­æ–‡æœ¬")
        
        # å¤„ç†å•ä¸ªæ–‡æœ¬ç‰‡æ®µ
        final_audio, final_sr = _process_single_segment(
            text_segment=text,
            refer_wav_path=refer_wav_path,
            prompt_text=prompt_text,
            prompt_language=prompt_language,
            text_language=text_language,
            top_k=top_k,
            top_p=top_p,
            temperature=temperature,
            speed=speed,
            inp_refs=inp_refs,
            sample_steps=sample_steps,
            if_sr=if_sr,
            spk=spk,
            gpu_index=selected_gpu
        )
    
    # 4. åŒ…è£…éŸ³é¢‘ä¸ºå­—èŠ‚æµ
    all_audio_bytes = BytesIO()
    
    # æ ¹æ®æ•°æ®ç±»å‹ç¼–ç éŸ³é¢‘
    if is_int32:
        audio_data_int = (final_audio * 2147483647).astype(np.int32)
    else:
        audio_data_int = (final_audio * 32767).astype(np.int16)
    
    # æ ¹æ®åª’ä½“ç±»å‹æ‰“åŒ…éŸ³é¢‘
    if media_type == "wav":
        audio_bytes = pack_wav(audio_data_int, final_sr)
    else:
        audio_bytes = pack_audio(all_audio_bytes, audio_data_int, final_sr)
    
    # 5. è¿”å›éŸ³é¢‘æ•°æ®
    if stream_mode == "normal":
        # æµå¼è¿”å›
        audio_bytes, audio_chunk = read_clean_buffer(audio_bytes)
        yield audio_chunk
    else:
        # ä¸€æ¬¡æ€§è¿”å›
        yield audio_bytes.getvalue()

def _process_single_segment(text_segment, refer_wav_path, prompt_text, prompt_language,
                           text_language, top_k, top_p, temperature, speed,
                           inp_refs, sample_steps, if_sr, spk, gpu_index):
    """
    å¤„ç†å•ä¸ªæ–‡æœ¬ç‰‡æ®µï¼ˆå†…éƒ¨å‡½æ•°ï¼Œç”¨äºå¹¶è¡Œå¤„ç†ï¼‰
    è¿”å›: (audio_data, sampling_rate)
    """
    try:
        global bigvgan_model, hifigan_model, sv_cn_model
        
        # 1. è·å–ç›®æ ‡GPU
        target_gpu = f"cuda:{gpu_index}"
        
        # 2. ç¡®ä¿æ¨¡å‹å·²åŠ è½½åˆ°ç›®æ ‡GPU
        ensure_model_loaded(spk, gpu_index)
        
        # 3. è·å–å¯¹åº”GPUä¸Šçš„æ¨¡å‹å®ä¾‹
        speaker = speaker_list[spk]
        if gpu_index == 0:
            model_instance = {
                "gpt": speaker.gpu0_gpt,
                "sovits": speaker.gpu0_sovits
            }
        elif gpu_index == 1:
            model_instance = {
                "gpt": speaker.gpu1_gpt,
                "sovits": speaker.gpu1_sovits
            }
        else:
            raise ValueError(f"Invalid GPU index: {gpu_index}")
        
        if model_instance["gpt"] is None or model_instance["sovits"] is None:
            raise ValueError(f"Model not loaded on GPU {gpu_index} for speaker {spk}")
        
        # 4. è·å–æ¨¡å‹å®ä¾‹å’Œé…ç½®
        infer_sovits = model_instance["sovits"]
        infer_gpt = model_instance["gpt"]
        
        vq_model = infer_sovits.vq_model
        hps = infer_sovits.hps
        version = vq_model.version
        
        t2s_model = infer_gpt.t2s_model
        max_sec = infer_gpt.max_sec
        
        # 5. å‚æ•°è°ƒæ•´ï¼ˆä¸åŸå§‹å‡½æ•°ç›¸åŒï¼‰
        if version == "v3":
            if sample_steps not in [4, 8, 16, 32, 64, 128]:
                sample_steps = 32
        elif version == "v4":
            if sample_steps not in [4, 8, 16, 32]:
                sample_steps = 8
        
        if if_sr and version != "v3":
            if_sr = False
        
        # 6. å‡†å¤‡å‚è€ƒéŸ³é¢‘ï¼ˆä¸åŸå§‹å‡½æ•°ç›¸åŒï¼Œä½†æŒ‡å®šç›®æ ‡GPUï¼‰
        prompt_text = prompt_text.strip("\n")
        if prompt_text[-1] not in splits:
            prompt_text += "ã€‚" if prompt_language != "en" else "."
        
        prompt_language, text_segment = prompt_language, text_segment.strip("\n")
        dtype = torch.float16 if is_half == True else torch.float32
        
        # åˆ›å»ºé™éŸ³ç‰‡æ®µï¼ˆä¸åŸå§‹å‡½æ•°ç›¸åŒï¼‰
        zero_wav = np.zeros(int(hps.data.sampling_rate * 0.3), dtype=np.float16 if is_half == True else np.float32)
        zero_wav_torch = torch.from_numpy(zero_wav)
        
        # 7. å‚è€ƒéŸ³é¢‘å¤„ç†ï¼ˆä¸åŸå§‹å‡½æ•°ç›¸åŒï¼Œä½†æŒ‡å®šç›®æ ‡GPUï¼‰
        with torch.no_grad():
            wav16k, sr = librosa.load(refer_wav_path, sr=16000)
            wav16k = torch.from_numpy(wav16k)
            
            if is_half == True:
                wav16k = wav16k.half().to(target_gpu)
                zero_wav_torch = zero_wav_torch.half().to(target_gpu)
            else:
                wav16k = wav16k.to(target_gpu)
                zero_wav_torch = zero_wav_torch.to(target_gpu)
            
            wav16k = torch.cat([wav16k, zero_wav_torch])
            
            # SSLæ¨¡å‹å¤„ç†
            ssl_device = None
            try:
                if hasattr(ssl_model, 'parameters'):
                    ssl_device = next(ssl_model.parameters()).device
                elif hasattr(ssl_model, 'device'):
                    ssl_device = ssl_model.device
            except Exception as e:
                ssl_device = torch.device("cpu")
            
            if ssl_device.type != "cpu":
                wav16k_for_ssl = wav16k.to(ssl_device)
                ssl_content = ssl_model.model(wav16k_for_ssl.unsqueeze(0))["last_hidden_state"].transpose(1, 2)
                ssl_content = ssl_content.to(target_gpu)
            else:
                wav16k_cpu = wav16k.cpu()
                ssl_content = ssl_model.model(wav16k_cpu.unsqueeze(0))["last_hidden_state"].transpose(1, 2)
                if is_half == True:
                    ssl_content = ssl_content.half()
                ssl_content = ssl_content.to(target_gpu)
            
            codes = vq_model.extract_latent(ssl_content)
            prompt_semantic = codes[0, 0]
            prompt = prompt_semantic.unsqueeze(0).to(target_gpu)
            
            # è·å–å‚è€ƒé¢‘è°±ï¼ˆæ ¹æ®æ¨¡å‹ç‰ˆæœ¬ï¼‰
            is_v2pro = version in {"v2Pro", "v2ProPlus"}
            if version not in {"v3", "v4"}:
                refers = []
                if is_v2pro:
                    sv_emb = []
                    if sv_cn_model == None:
                        init_sv_cn()
                if inp_refs:
                    for path in inp_refs:
                        try:
                            refer, audio_tensor = get_spepc_for_gpu(hps, path, dtype, target_gpu, is_v2pro)
                            refers.append(refer)
                            if is_v2pro:
                                sv_emb.append(sv_cn_model.compute_embedding3(audio_tensor))
                        except Exception as e:
                            logger.error(e)
                if len(refers) == 0:
                    refers, audio_tensor = get_spepc_for_gpu(hps, refer_wav_path, dtype, target_gpu, is_v2pro)
                    refers = [refers]
                    if is_v2pro:
                        sv_emb = [sv_cn_model.compute_embedding3(audio_tensor)]
            else:
                refer, audio_tensor = get_spepc_for_gpu(hps, refer_wav_path, dtype, target_gpu)
        
        # 8. æ–‡æœ¬å¤„ç†ï¼ˆä¸åŸå§‹å‡½æ•°ç›¸åŒï¼Œä½†æŒ‡å®šç›®æ ‡GPUï¼‰
        texts = text_segment.split("\n")
        audio_opt = []  # å­˜å‚¨æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ
        
        for text in texts:
            if only_punc(text):
                continue
            
            if text[-1] not in splits:
                text += "ã€‚" if text_language != "en" else "."
            
            # è·å–éŸ³ç´ å’ŒBERTç‰¹å¾
            phones1, bert1, norm_text1 = get_phones_and_bert(prompt_text, prompt_language, version)
            phones2, bert2, norm_text2 = get_phones_and_bert(text, text_language, version)
            
            # å°†BERTç‰¹å¾è½¬ç§»åˆ°ç›®æ ‡GPU
            bert = torch.cat([bert1, bert2], 1)
            bert = bert.to(target_gpu).unsqueeze(0)
            
            all_phoneme_ids = torch.LongTensor(phones1 + phones2).to(target_gpu).unsqueeze(0)
            all_phoneme_len = torch.tensor([all_phoneme_ids.shape[-1]]).to(target_gpu)
            
            # 9. GPTæ¨ç†ï¼ˆä¸åŸå§‹å‡½æ•°ç›¸åŒï¼Œä½†æŒ‡å®šç›®æ ‡GPUï¼‰
            with torch.no_grad():
                pred_semantic, idx = t2s_model.model.infer_panel(
                    all_phoneme_ids,
                    all_phoneme_len,
                    prompt,
                    bert,
                    top_k=top_k,
                    top_p=top_p,
                    temperature=temperature,
                    early_stop_num=hz * max_sec,
                )
                pred_semantic = pred_semantic[:, -idx:].unsqueeze(0)
            
            # 10. SoVITSè§£ç ï¼ˆæ ¹æ®æ¨¡å‹ç‰ˆæœ¬ä¸åŒï¼‰
            if version not in {"v3", "v4"}:
                # v1/v2/v2Pro ç‰ˆæœ¬
                if is_v2pro:
                    audio = (
                        vq_model.decode(
                            pred_semantic,
                            torch.LongTensor(phones2).to(target_gpu).unsqueeze(0),
                            refers,
                            speed=speed,
                            sv_emb=sv_emb,
                        )
                        .detach()
                        .cpu()
                        .numpy()[0, 0]
                    )
                else:
                    audio = (
                        vq_model.decode(
                            pred_semantic, 
                            torch.LongTensor(phones2).to(target_gpu).unsqueeze(0), 
                            refers, 
                            speed=speed
                        )
                        .detach()
                        .cpu()
                        .numpy()[0, 0]
                    )
            else:
                # v3/v4 ç‰ˆæœ¬
                phoneme_ids0 = torch.LongTensor(phones1).to(target_gpu).unsqueeze(0)
                phoneme_ids1 = torch.LongTensor(phones2).to(target_gpu).unsqueeze(0)
                
                fea_ref, ge = vq_model.decode_encp(prompt.unsqueeze(0), phoneme_ids0, refer)
                
                # åŠ è½½å‚è€ƒéŸ³é¢‘ç”¨äºmelé¢‘è°±
                ref_audio, sr = torchaudio.load(refer_wav_path)
                ref_audio = ref_audio.to(target_gpu).float()
                if ref_audio.shape[0] == 2:
                    ref_audio = ref_audio.mean(0).unsqueeze(0)
                
                tgt_sr = 24000 if version == "v3" else 32000
                if sr != tgt_sr:
                    ref_audio = resample(ref_audio, sr, tgt_sr, target_gpu)
                
                mel2 = mel_fn(ref_audio) if version == "v3" else mel_fn_v4(ref_audio)
                mel2 = norm_spec(mel2)
                
                # ============ v3/v4 åˆ†å—è§£ç è¿‡ç¨‹ ============
                T_min = min(mel2.shape[2], fea_ref.shape[2])
                mel2 = mel2[:, :, :T_min]
                fea_ref = fea_ref[:, :, :T_min]
                Tref = 468 if version == "v3" else 500
                Tchunk = 934 if version == "v3" else 1000
                if T_min > Tref:
                    mel2 = mel2[:, :, -Tref:]
                    fea_ref = fea_ref[:, :, -Tref:]
                    T_min = Tref

                chunk_len = Tchunk - T_min
                mel2 = mel2.to(dtype)
                fea_todo, ge = vq_model.decode_encp(pred_semantic, phoneme_ids1, refer, ge, speed)

                # åˆ†å—è§£ç 
                cfm_resss = []
                idx = 0
                while 1:
                    fea_todo_chunk = fea_todo[:, :, idx: idx + chunk_len]
                    if fea_todo_chunk.shape[-1] == 0:
                        break
                    idx += chunk_len
                    fea = torch.cat([fea_ref, fea_todo_chunk], 2).transpose(2, 1)
                    cfm_res = vq_model.cfm.inference(
                        fea, torch.LongTensor([fea.size(1)]).to(target_gpu), mel2, sample_steps, inference_cfg_rate=0
                    )
                    cfm_res = cfm_res[:, :, mel2.shape[2]:]
                    mel2 = cfm_res[:, :, -T_min:]
                    fea_ref = fea_todo_chunk[:, :, -T_min:]
                    cfm_resss.append(cfm_res)

                cfm_res = torch.cat(cfm_resss, 2)
                cfm_res = denorm_spec(cfm_res)

                # æ ¹æ®ç‰ˆæœ¬é€‰æ‹©å£°ç å™¨
                if version == "v3":
                    # ç¡®ä¿ bigvgan_model åœ¨ç›®æ ‡ GPU ä¸Š
                    if bigvgan_model is None:
                        init_bigvgan(target_gpu)
                    else:
                        try:
                            current_device = next(bigvgan_model.parameters()).device
                            if str(current_device) != target_gpu:
                                bigvgan_model = bigvgan_model.to(target_gpu)
                        except StopIteration:
                            bigvgan_model = bigvgan_model.to(target_gpu)
                    vocoder_model = bigvgan_model
                else:  # v4
                    # ç¡®ä¿ hifigan_model åœ¨ç›®æ ‡ GPU ä¸Š
                    if hifigan_model is None:
                        init_hifigan(target_gpu)
                    else:
                        try:
                            current_device = next(hifigan_model.parameters()).device
                            if str(current_device) != target_gpu:
                                hifigan_model = hifigan_model.to(target_gpu)
                        except StopIteration:
                            hifigan_model = hifigan_model.to(target_gpu)
                    vocoder_model = hifigan_model

                # ç”ŸæˆéŸ³é¢‘
                with torch.inference_mode():
                    # ç¡®ä¿è¾“å…¥æ•°æ®ç±»å‹ä¸å£°ç å™¨ä¸€è‡´
                    model_dtype = next(vocoder_model.parameters()).dtype
                    if cfm_res.dtype != model_dtype:
                        cfm_res = cfm_res.to(model_dtype)
                    
                    wav_gen = vocoder_model(cfm_res)
                    audio = wav_gen[0][0].cpu().detach().numpy()
            
            # 11. éŸ³é¢‘å½’ä¸€åŒ–ï¼ˆæ¯è¡Œç‹¬ç«‹å½’ä¸€åŒ–ï¼Œé˜²æ­¢å‰Šæ³¢ï¼‰
            max_audio = np.abs(audio).max()
            if max_audio > 1:
                audio = audio / max_audio
            
            # æ·»åŠ åˆ°éŸ³é¢‘åˆ—è¡¨
            audio_opt.append(audio)
            audio_opt.append(zero_wav)  # æ·»åŠ é™éŸ³æ®µ
        
        # 12. åˆå¹¶æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ
        if audio_opt:
            combined_audio = np.concatenate(audio_opt, axis=0)
            
            # æœ€ç»ˆå½’ä¸€åŒ–
            max_audio = np.abs(combined_audio).max()
            if max_audio > 1.0:
                combined_audio = combined_audio / max_audio
            
            # 13. ç¡®å®šé‡‡æ ·ç‡ï¼ˆæ ¹æ®æ¨¡å‹ç‰ˆæœ¬ï¼‰
            if version in {"v1", "v2", "v2Pro", "v2ProPlus"}:
                sr = 32000
            elif version == "v3":
                sr = 48000 if if_sr else 24000
            else:  # v4
                sr = 48000
            
            # 14. è¶…åˆ†å¤„ç†ï¼ˆä»…v3ä¸”å¯ç”¨è¶…åˆ†ï¼‰
            if if_sr and version == "v3" and sr == 24000:
                audio_opt_tensor = torch.from_numpy(combined_audio).float().to(target_gpu)
                audio_opt_tensor, sr = audio_sr_for_gpu(audio_opt_tensor.unsqueeze(0), sr, target_gpu)
                combined_audio = audio_opt_tensor.cpu().numpy()[0]
                
                # è¶…åˆ†åå†æ¬¡å½’ä¸€åŒ–
                max_audio = np.abs(combined_audio).max()
                if max_audio > 1.0:
                    combined_audio = combined_audio / max_audio
                sr = 48000
            combined_audio = _validate_audio_output(combined_audio, sr)
            return combined_audio, sr
        else:
            # å¦‚æœæ²¡æœ‰ç”ŸæˆéŸ³é¢‘ï¼Œè¿”å›é™éŸ³
            silence = np.zeros(16000, dtype=np.float32)
            return silence, 16000
            
    except Exception as e:
        logger.error(f"âŒ GPU{gpu_index} å¤„ç†å¤±è´¥: {e}")
        raise

def get_spepc_for_gpu(hps, filename, dtype, target_gpu, is_v2pro=False):
    """
    ä¸ºæŒ‡å®šGPUè·å–é¢‘è°±ï¼ˆä¿®æ”¹è‡ªåŸget_spepcå‡½æ•°ï¼‰
    """
    sr1 = int(hps.data.sampling_rate)
    audio, sr0 = torchaudio.load(filename)
    
    # ç¡®ä¿éŸ³é¢‘åœ¨ç›®æ ‡GPUä¸Š
    audio = audio.to(target_gpu)
    
    if sr0 != sr1:
        if audio.shape[0] == 2:
            audio = audio.mean(0).unsqueeze(0)
        audio = resample(audio, sr0, sr1, target_gpu)
    else:
        if audio.shape[0] == 2:
            audio = audio.mean(0).unsqueeze(0)
    
    maxx = audio.abs().max()
    if maxx > 1:
        audio /= min(2, maxx)
    
    spec = spectrogram_torch(
        audio,
        hps.data.filter_length,
        hps.data.sampling_rate,
        hps.data.hop_length,
        hps.data.win_length,
        center=False,
    )
    spec = spec.to(dtype)
    
    if is_v2pro == True:
        audio = resample(audio, sr1, 16000, target_gpu).to(dtype)
    
    return spec, audio


def audio_sr_for_gpu(audio, sr, target_gpu):
    """
    ä¸ºæŒ‡å®šGPUè¿›è¡ŒéŸ³é¢‘è¶…åˆ†
    """
    global sr_model
    if sr_model == None:
        from tools.audio_sr import AP_BWE
        try:
            sr_model = AP_BWE(target_gpu, DictToAttrRecursive)
        except FileNotFoundError:
            logger.info("ä½ æ²¡æœ‰ä¸‹è½½è¶…åˆ†æ¨¡å‹çš„å‚æ•°ï¼Œå› æ­¤ä¸è¿›è¡Œè¶…åˆ†ã€‚")
            return audio.cpu().detach().numpy(), sr
    return sr_model(audio, sr)

def _get_model_specific_params(version, if_sr):
    """
    æ ¹æ®æ¨¡å‹ç‰ˆæœ¬è·å–ç‰¹å®šå‚æ•°
    è¿”å›: (sample_rate, needs_vocoder, vocoder_type)
    """
    if version in {"v1", "v2", "v2Pro", "v2ProPlus"}:
        sr = 32000
        needs_vocoder = False
        vocoder_type = None
    elif version == "v3":
        if if_sr:
            sr = 48000  # è¶…åˆ†å
        else:
            sr = 24000  # åŸå§‹
        needs_vocoder = True
        vocoder_type = "bigvgan"
    else:  # v4
        sr = 48000
        needs_vocoder = True
        vocoder_type = "hifigan"
    
    return sr, needs_vocoder, vocoder_type

def _get_ssl_content(wav16k, target_gpu):
    """
    è·å–SSLå†…å®¹ï¼Œå¤„ç†è®¾å¤‡é—´æ•°æ®ä¼ è¾“
    """
    ssl_device = None
    try:
        if hasattr(ssl_model, 'parameters'):
            ssl_device = next(ssl_model.parameters()).device
        elif hasattr(ssl_model, 'device'):
            ssl_device = ssl_model.device
    except Exception as e:
        logger.warning(f"æ— æ³•è·å–SSLæ¨¡å‹è®¾å¤‡ï¼Œé»˜è®¤ä½¿ç”¨CPU: {e}")
        ssl_device = torch.device("cpu")
    
    # æ ¹æ®SSLæ¨¡å‹è®¾å¤‡å¤„ç†
    ssl_content = _get_ssl_content(wav16k, target_gpu)
    
    return ssl_content

def check_gpu_memory(gpu_index=0):
    """
    æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
    """
    if not torch.cuda.is_available():
        return 0, 0
    
    try:
        torch.cuda.set_device(gpu_index)
        allocated = torch.cuda.memory_allocated(gpu_index) / 1024**3  # GB
        reserved = torch.cuda.memory_reserved(gpu_index) / 1024**3   # GB
        total = torch.cuda.get_device_properties(gpu_index).total_memory / 1024**3
        
        free = total - allocated
        
        logger.debug(f"GPU{gpu_index}: å·²ç”¨ {allocated:.2f}GB / æ€»è®¡ {total:.2f}GB, ç©ºé—² {free:.2f}GB")
        return free, total
    except Exception as e:
        logger.warning(f"æ£€æŸ¥GPU{gpu_index}å†…å­˜å¤±è´¥: {e}")
        return 0, 0

def _resample_audio_if_needed(audio_data, orig_sr, target_sr):
    """
    å¦‚æœéœ€è¦ï¼Œå¯¹éŸ³é¢‘è¿›è¡Œé‡é‡‡æ ·
    """
    if orig_sr == target_sr:
        return audio_data
    
    try:
        # ä½¿ç”¨librosaè¿›è¡Œé«˜è´¨é‡é‡é‡‡æ ·
        resampled = librosa.resample(audio_data, orig_sr=orig_sr, target_sr=target_sr)
        logger.debug(f"éŸ³é¢‘é‡é‡‡æ ·: {orig_sr}Hz -> {target_sr}Hz")
        return resampled
    except Exception as e:
        logger.error(f"éŸ³é¢‘é‡é‡‡æ ·å¤±è´¥: {e}")
        # å¦‚æœé‡é‡‡æ ·å¤±è´¥ï¼Œè¿”å›åŸå§‹éŸ³é¢‘å¹¶è®°å½•è­¦å‘Š
        return audio_data

def _safe_model_load(model_func, *args, **kwargs):
    """
    å®‰å…¨çš„æ¨¡å‹åŠ è½½ï¼Œå¸¦é‡è¯•æœºåˆ¶
    """
    max_retries = 2
    for attempt in range(max_retries):
        try:
            return model_func(*args, **kwargs)
        except torch.cuda.OutOfMemoryError:
            logger.warning(f"å†…å­˜ä¸è¶³ï¼Œå°è¯• {attempt+1}/{max_retries}...")
            torch.cuda.empty_cache()
            if attempt == max_retries - 1:
                raise
            time.sleep(1)  # ç­‰å¾…1ç§’å†é‡è¯•
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

def _validate_audio_output(audio_data, sample_rate):
    """
    éªŒè¯éŸ³é¢‘è¾“å‡ºæ˜¯å¦æœ‰æ•ˆ
    """
    if audio_data is None or len(audio_data) == 0:
        raise ValueError("ç”Ÿæˆçš„éŸ³é¢‘æ•°æ®ä¸ºç©º")
    
    if sample_rate <= 0:
        raise ValueError(f"æ— æ•ˆçš„é‡‡æ ·ç‡: {sample_rate}")
    
    # æ£€æŸ¥éŸ³é¢‘æ•°æ®æ˜¯å¦åŒ…å«æ— æ•ˆå€¼
    if np.any(np.isnan(audio_data)) or np.any(np.isinf(audio_data)):
        logger.warning("éŸ³é¢‘æ•°æ®åŒ…å«NaNæˆ–Infå€¼ï¼Œå°è¯•ä¿®å¤...")
        audio_data = np.nan_to_num(audio_data, nan=0.0, posinf=1.0, neginf=-1.0)
    
    # æ£€æŸ¥å¹…åº¦æ˜¯å¦è¿‡å¤§
    max_amplitude = np.abs(audio_data).max()
    if max_amplitude > 10.0:  # æ˜æ˜¾è¿‡å¤§
        logger.warning(f"éŸ³é¢‘å¹…åº¦è¿‡å¤§ ({max_amplitude:.2f})ï¼Œè¿›è¡Œå½’ä¸€åŒ–")
        audio_data = audio_data / max_amplitude
    
    return audio_data
    
def handle_control(command):
    if command == "restart":
        os.execl(g_config.python_exec, g_config.python_exec, *sys.argv)
    elif command == "exit":
        os.kill(os.getpid(), signal.SIGTERM)
        exit(0)


def handle_change(path, text, language):
    if is_empty(path, text, language):
        return JSONResponse(
            {"code": 400, "message": 'ç¼ºå°‘ä»»æ„ä¸€é¡¹ä»¥ä¸‹å‚æ•°: "path", "text", "language"'}, status_code=400
        )

    if path != "" or path is not None:
        default_refer.path = path
    if text != "" or text is not None:
        default_refer.text = text
    if language != "" or language is not None:
        default_refer.language = language

    logger.info(f"å½“å‰é»˜è®¤å‚è€ƒéŸ³é¢‘è·¯å¾„: {default_refer.path}")
    logger.info(f"å½“å‰é»˜è®¤å‚è€ƒéŸ³é¢‘æ–‡æœ¬: {default_refer.text}")
    logger.info(f"å½“å‰é»˜è®¤å‚è€ƒéŸ³é¢‘è¯­ç§: {default_refer.language}")
    logger.info(f"is_ready: {default_refer.is_ready()}")

    return JSONResponse({"code": 0, "message": "Success"}, status_code=200)


def unload_model(speaker_id):
    if speaker_id in speaker_list and speaker_list[speaker_id].gpt is not None:
        speaker_list[speaker_id].gpt = None
        speaker_list[speaker_id].sovits = None
        torch.cuda.empty_cache()


def get_speaker_gpt_model(speaker_id, gpu_index=0):
    """è·å–æŒ‡å®šè¯´è¯äººåœ¨æŒ‡å®šGPUä¸Šçš„GPTæ¨¡å‹"""
    speaker = speaker_list[speaker_id]
    if gpu_index == 0:
        return speaker.gpu0_gpt or speaker.gpt  # å›é€€åˆ°å…¼å®¹å­—æ®µ
    elif gpu_index == 1:
        return speaker.gpu1_gpt
    else:
        raise ValueError(f"Invalid GPU index: {gpu_index}")

def get_speaker_sovits_model(speaker_id, gpu_index=0):
    """è·å–æŒ‡å®šè¯´è¯äººåœ¨æŒ‡å®šGPUä¸Šçš„Sovitsæ¨¡å‹"""
    speaker = speaker_list[speaker_id]
    if gpu_index == 0:
        return speaker.gpu0_sovits or speaker.sovits  # å›é€€åˆ°å…¼å®¹å­—æ®µ
    elif gpu_index == 1:
        return speaker.gpu1_sovits
    else:
        raise ValueError(f"Invalid GPU index: {gpu_index}")
    
def split_long_text(text, threshold=long_text_threshold):
    """
    æ™ºèƒ½æ‹†åˆ†é•¿æ–‡æœ¬ï¼Œå°½é‡åœ¨è‡ªç„¶åœé¡¿å¤„æ‹†åˆ†
    è¿”å›ï¼š(is_long, segments)
    - is_long: æ˜¯å¦ä¸ºé•¿æ–‡æœ¬
    - segments: æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨ï¼Œé•¿æ–‡æœ¬æ—¶ä¸º2æ®µï¼ŒçŸ­æ–‡æœ¬æ—¶ä¸º1æ®µ
    """
    # å•GPUç¯å¢ƒä¸è¿›è¡Œå¹¶è¡Œæ‹†åˆ†
    if not IS_MULTI_GPU:
        return False, [text]
        
    # å¤šGPUç¯å¢ƒæ‹†åˆ† 
    if len(text) <= threshold:
        return False, [text]
    
    # å¯»æ‰¾æœ€ä½³çš„æ‹†åˆ†ç‚¹ï¼ˆåœ¨å¥å·ã€é—®å·ã€æ„Ÿå¹å·ã€é€—å·ç­‰ä½ç½®ï¼‰
    split_positions = []
    for i in range(len(text)):
        if i > threshold * 0.3 and i < len(text) - threshold * 0.3:
            if text[i] in 'ã€‚ï¼ï¼Ÿ.!?ï¼›;ï¼Œ,':
                split_positions.append(i)
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„æ ‡ç‚¹ï¼Œåœ¨é˜ˆå€¼ä½ç½®å¼ºåˆ¶æ‹†åˆ†
    if not split_positions:
        split_pos = min(threshold, len(text) - 1)
    else:
        # é€‰æ‹©æœ€æ¥è¿‘ä¸­é—´ä½ç½®çš„æ ‡ç‚¹
        mid_point = len(text) // 2
        split_pos = min(split_positions, key=lambda x: abs(x - mid_point))
    
    # ç¡®ä¿æ‹†åˆ†ç‚¹ä¸æ˜¯æœ€åä¸€ä¸ªå­—ç¬¦
    split_pos = min(split_pos, len(text) - 5)
    
    return True, [text[:split_pos+1], text[split_pos+1:]]

# 6. ä¿®æ”¹ handle å‡½æ•°ï¼Œè°ƒæ•´ prompt å­—æ®µè®¿é—®
def handle(
    refer_wav_path,
    prompt_text,
    prompt_language,
    text,
    text_language,
    cut_punc,
    top_k,
    top_p,
    temperature,
    speed,
    inp_refs,
    sample_steps,
    if_sr,
    speaker_id="default"
):
    if speaker_id not in speaker_list:
        return JSONResponse({"code": 400, "message": f"speaker_id: {speaker_id} not found"}, status_code=400)

    # ä½¿ç”¨ speaker_list ä¸­å®šä¹‰çš„é»˜è®¤å€¼
    if (
        refer_wav_path == "" or refer_wav_path is None
        or prompt_text == "" or prompt_text is None
        or prompt_language == "" or prompt_language is None
    ):
        refer_wav_path = speaker_list[speaker_id].prompt["ref_audio"] if refer_wav_path in ["", None] else refer_wav_path
        prompt_text = speaker_list[speaker_id].prompt["prompt_text"] if prompt_text in ["", None] else prompt_text
        prompt_language = speaker_list[speaker_id].prompt["prompt_lang"] if prompt_language in ["", None] else prompt_language

        # å¦‚æœä»ç„¶ç¼ºå°‘å¿…è¦å‚æ•°ï¼Œå°è¯•ä½¿ç”¨å…¨å±€é»˜è®¤å‚è€ƒéŸ³é¢‘
        if not is_full(refer_wav_path, prompt_text, prompt_language):
            refer_wav_path = default_refer.path if refer_wav_path in ["", None] else refer_wav_path
            prompt_text = default_refer.text if prompt_text in ["", None] else prompt_text
            prompt_language = default_refer.language if prompt_language in ["", None] else prompt_language
            if not default_refer.is_ready():
                return JSONResponse({"code": 400, "message": "æœªæŒ‡å®šå‚è€ƒéŸ³é¢‘ä¸”æ¥å£æ— é¢„è®¾"}, status_code=400)

    if sample_steps not in [4, 8, 16, 32]:
        sample_steps = 32

    if cut_punc is None:
        text = cut_text(text, default_cut_punc)
    else:
        text = cut_text(text, cut_punc)

    # éªŒè¯ prompt_language å’Œ text_language
    prompt_language = dict_language.get(prompt_language.lower(), prompt_language)
    text_language = dict_language.get(text_language.lower(), text_language)
    supported_languages = ["all_zh", "all_yue", "en", "all_ja", "all_ko", "zh", "yue", "ja", "ko", "auto", "auto_yue"]
    if prompt_language not in supported_languages:
        return JSONResponse({"code": 400, "message": f"prompt_language: {prompt_language} is not supported"}, status_code=400)
    if text_language not in supported_languages:
        return JSONResponse({"code": 400, "message": f"text_language: {text_language} is not supported"}, status_code=400)

    return StreamingResponse(
        get_tts_wav(
            refer_wav_path,
            prompt_text,
            prompt_language,
            text,
            text_language,
            top_k,
            top_p,
            temperature,
            speed,
            inp_refs,
            sample_steps,
            if_sr,
            spk=speaker_id
        ),
        media_type="audio/" + media_type,
    )


# --------------------------------
# åˆå§‹åŒ–éƒ¨åˆ†
# --------------------------------

dict_language = {
    "ä¸­æ–‡": "all_zh",
    "ç²¤è¯­": "all_yue",
    "è‹±æ–‡": "en",
    "æ—¥æ–‡": "all_ja",
    "éŸ©æ–‡": "all_ko",
    "ä¸­è‹±æ··åˆ": "zh",
    "ç²¤è‹±æ··åˆ": "yue",
    "æ—¥è‹±æ··åˆ": "ja",
    "éŸ©è‹±æ··åˆ": "ko",
    "å¤šè¯­ç§æ··åˆ": "auto",  # å¤šè¯­ç§å¯åŠ¨åˆ‡åˆ†è¯†åˆ«è¯­ç§
    "å¤šè¯­ç§æ··åˆ(ç²¤è¯­)": "auto_yue",
    "all_zh": "all_zh",
    "all_yue": "all_yue",
    "en": "en",
    "all_ja": "all_ja",
    "all_ko": "all_ko",
    "zh": "zh",
    "yue": "yue",
    "ja": "ja",
    "ko": "ko",
    "auto": "auto",
    "auto_yue": "auto_yue",
}

# logger
logging.config.dictConfig(uvicorn.config.LOGGING_CONFIG)
logger = logging.getLogger("uvicorn")

# è·å–é…ç½®
g_config = global_config.Config()

# è·å–å‚æ•°
parser = argparse.ArgumentParser(description="GPT-SoVITS api")

parser.add_argument("-s", "--sovits_path", type=str, default=g_config.sovits_path, help="SoVITSæ¨¡å‹è·¯å¾„")
parser.add_argument("-g", "--gpt_path", type=str, default=g_config.gpt_path, help="GPTæ¨¡å‹è·¯å¾„")
parser.add_argument("-dr", "--default_refer_path", type=str, default="", help="é»˜è®¤å‚è€ƒéŸ³é¢‘è·¯å¾„")
parser.add_argument("-dt", "--default_refer_text", type=str, default="", help="é»˜è®¤å‚è€ƒéŸ³é¢‘æ–‡æœ¬")
parser.add_argument("-dl", "--default_refer_language", type=str, default="", help="é»˜è®¤å‚è€ƒéŸ³é¢‘è¯­ç§")
parser.add_argument("-d", "--device", type=str, default=g_config.infer_device, help="cuda / cpu")
parser.add_argument("-a", "--bind_addr", type=str, default="0.0.0.0", help="default: 0.0.0.0")
parser.add_argument("-p", "--port", type=int, default=g_config.api_port, help="default: 9880")
parser.add_argument(
    "-fp", "--full_precision", action="store_true", default=False, help="è¦†ç›–config.is_halfä¸ºFalse, ä½¿ç”¨å…¨ç²¾åº¦"
)
parser.add_argument(
    "-hp", "--half_precision", action="store_true", default=False, help="è¦†ç›–config.is_halfä¸ºTrue, ä½¿ç”¨åŠç²¾åº¦"
)
# boolå€¼çš„ç”¨æ³•ä¸º `python ./api.py -fp ...`
# æ­¤æ—¶ full_precision==True, half_precision==False
parser.add_argument("-sm", "--stream_mode", type=str, default="close", help="æµå¼è¿”å›æ¨¡å¼, close / normal / keepalive")
parser.add_argument("-mt", "--media_type", type=str, default="wav", help="éŸ³é¢‘ç¼–ç æ ¼å¼, wav / ogg / aac")
parser.add_argument("-st", "--sub_type", type=str, default="int16", help="éŸ³é¢‘æ•°æ®ç±»å‹, int16 / int32")
parser.add_argument("-cp", "--cut_punc", type=str, default="", help="æ–‡æœ¬åˆ‡åˆ†ç¬¦å·è®¾å®š, ç¬¦å·èŒƒå›´,.;?!ã€ï¼Œã€‚ï¼Ÿï¼ï¼›ï¼šâ€¦")
# åˆ‡å‰²å¸¸ç”¨åˆ†å¥ç¬¦ä¸º `python ./api.py -cp ".?!ã€‚ï¼Ÿï¼"`
parser.add_argument("-hb", "--hubert_path", type=str, default=g_config.cnhubert_path, help="è¦†ç›–config.cnhubert_path")
parser.add_argument("-b", "--bert_path", type=str, default=g_config.bert_path, help="è¦†ç›–config.bert_path")
parser.add_argument("-mm", "--max_models", type=int, default=3, help="æœ€å¤§åŒæ—¶åŠ è½½æ¨¡å‹æ•°é‡")
parser.add_argument("-ltt", "--long_text_threshold", type=int, default=70, help="é•¿æ–‡æœ¬ç•Œé™")
parser.add_argument("--sd", "--subdomain", type=str, default=None, help="æŒ‡å®šéš§é“ä½¿ç”¨çš„å›ºå®šå­åŸŸå (ä¾‹å¦‚: your-name)")
# æ·»åŠ æ˜¯å¦åˆ›å»ºå…¬å¼€é“¾æ¥çš„å‚æ•°
parser.add_argument("--public", action="store_true", default=False,help="æ˜¯å¦åˆ›å»ºå…¬å¼€é“¾æ¥ (é»˜è®¤ä¸åˆ›å»º)")

args = parser.parse_args()
sovits_path = args.sovits_path
gpt_path = args.gpt_path
device = args.device
port = args.port
host = args.bind_addr
cnhubert_base_path = args.hubert_path
bert_path = args.bert_path
default_cut_punc = args.cut_punc
max_models = args.max_models
long_text_threshold = args.long_text_threshold

# åº”ç”¨å‚æ•°é…ç½®
default_refer = DefaultRefer(args.default_refer_path, args.default_refer_text, args.default_refer_language)

# æ¨¡å‹è·¯å¾„æ£€æŸ¥
if sovits_path == "":
    sovits_path = g_config.pretrained_sovits_path
    logger.warning(f"æœªæŒ‡å®šSoVITSæ¨¡å‹è·¯å¾„, fallbackåå½“å‰å€¼: {sovits_path}")
if gpt_path == "":
    gpt_path = g_config.pretrained_gpt_path
    logger.warning(f"æœªæŒ‡å®šGPTæ¨¡å‹è·¯å¾„, fallbackåå½“å‰å€¼: {gpt_path}")

# æŒ‡å®šé»˜è®¤å‚è€ƒéŸ³é¢‘, è°ƒç”¨æ–¹ æœªæä¾›/æœªç»™å…¨ å‚è€ƒéŸ³é¢‘å‚æ•°æ—¶ä½¿ç”¨
if default_refer.path == "" or default_refer.text == "" or default_refer.language == "":
    default_refer.path, default_refer.text, default_refer.language = "", "", ""
    logger.info("æœªæŒ‡å®šé»˜è®¤å‚è€ƒéŸ³é¢‘")
else:
    logger.info(f"é»˜è®¤å‚è€ƒéŸ³é¢‘è·¯å¾„: {default_refer.path}")
    logger.info(f"é»˜è®¤å‚è€ƒéŸ³é¢‘æ–‡æœ¬: {default_refer.text}")
    logger.info(f"é»˜è®¤å‚è€ƒéŸ³é¢‘è¯­ç§: {default_refer.language}")

# è·å–åŠç²¾åº¦
is_half = g_config.is_half
if args.full_precision:
    is_half = False
if args.half_precision:
    is_half = True
if args.full_precision and args.half_precision:
    is_half = g_config.is_half  # ç‚’é¥­fallback
logger.info(f"åŠç²¾: {is_half}")

# æµå¼è¿”å›æ¨¡å¼
if args.stream_mode.lower() in ["normal", "n"]:
    stream_mode = "normal"
    logger.info("æµå¼è¿”å›å·²å¼€å¯")
else:
    stream_mode = "close"

# éŸ³é¢‘ç¼–ç æ ¼å¼
if args.media_type.lower() in ["aac", "ogg"]:
    media_type = args.media_type.lower()
elif stream_mode == "close":
    media_type = "wav"
else:
    media_type = "ogg"
logger.info(f"ç¼–ç æ ¼å¼: {media_type}")

# éŸ³é¢‘æ•°æ®ç±»å‹
if args.sub_type.lower() == "int32":
    is_int32 = True
    logger.info("æ•°æ®ç±»å‹: int32")
else:
    is_int32 = False
    logger.info("æ•°æ®ç±»å‹: int16")

# åˆå§‹åŒ–æ¨¡å‹
cnhubert.cnhubert_base_path = cnhubert_base_path
tokenizer = AutoTokenizer.from_pretrained(bert_path)
bert_model = AutoModelForMaskedLM.from_pretrained(bert_path)
ssl_model = cnhubert.get_model()
if is_half:
    bert_model = bert_model.half().to(device)
    ssl_model = ssl_model.half().to(device)
else:
    bert_model = bert_model.to(device)
    ssl_model = ssl_model.to(device)
change_gpt_sovits_weights(gpt_path=gpt_path, sovits_path=sovits_path)


# 1. ä¿®æ”¹ speaker_list åˆå§‹åŒ–ï¼Œæ·»åŠ é»˜è®¤ ref_audioã€prompt_text å’Œ prompt_lang
#n_speaker#S
speaker_list = {}
#n_speaker#E

# --------------------------------
# æ¥å£éƒ¨åˆ†
# --------------------------------
app = FastAPI()

# åœ¨æ¥å£éƒ¨åˆ†æ·»åŠ  /voice/speakers æ¥å£
@app.get("/voice/speakers")
async def get_speakers():
    # åªè¿”å›é€»è¾‘è¯´è¯äººåˆ—è¡¨ï¼Œä¸æš´éœ²å†…éƒ¨çš„åŒGPUç»“æ„
    speakers = [{"name": speaker_id} for speaker_id in speaker_list.keys()]
    return JSONResponse({"GPT-SOVITS": speakers}, status_code=200)

# 6. ä¿®æ”¹ set_model æ¥å£ï¼Œæ”¯æŒ speaker_id
@app.post("/set_model")
async def set_model(request: Request):
    json_post_raw = await request.json()
    return change_gpt_sovits_weights(
        gpt_path=json_post_raw.get("gpt_model_path"),
        sovits_path=json_post_raw.get("sovits_model_path"),
        speaker_id=json_post_raw.get("speaker_id", "default")
    )

@app.get("/set_model")
async def set_model(
    gpt_model_path: str = None,
    sovits_model_path: str = None,
    speaker_id: str = "default"
):
    return change_gpt_sovits_weights(gpt_path=gpt_model_path, sovits_path=sovits_model_path, speaker_id=speaker_id)

@app.post("/control")
async def control(request: Request):
    json_post_raw = await request.json()
    return handle_control(json_post_raw.get("command"))


@app.get("/control")
async def control(command: str = None):
    return handle_control(command)


@app.post("/change_refer")
async def change_refer(request: Request):
    json_post_raw = await request.json()
    return handle_change(
        json_post_raw.get("refer_wav_path"), json_post_raw.get("prompt_text"), json_post_raw.get("prompt_language")
    )


@app.get("/change_refer")
async def change_refer(refer_wav_path: str = None, prompt_text: str = None, prompt_language: str = None):
    return handle_change(refer_wav_path, prompt_text, prompt_language)


# 4. ä¿®æ”¹ tts_endpoint POST æ¥å£ï¼Œæ”¯æŒ speaker_id
@app.post("/")
async def tts_endpoint(request: Request):
    json_post_raw = await request.json()
    return handle(
        json_post_raw.get("refer_wav_path"),
        json_post_raw.get("prompt_text"),
        json_post_raw.get("prompt_language"),
        json_post_raw.get("text"),
        json_post_raw.get("text_language"),
        json_post_raw.get("cut_punc"),
        json_post_raw.get("top_k", 15),
        json_post_raw.get("top_p", 1.0),
        json_post_raw.get("temperature", 1.0),
        json_post_raw.get("speed", 1.0),
        json_post_raw.get("inp_refs", []),
        json_post_raw.get("sample_steps", 32),
        json_post_raw.get("if_sr", False),
        json_post_raw.get("speaker_id", "default")
    )


# 3. ä¿®æ”¹ tts_endpoint GET æ¥å£ï¼Œæ·»åŠ  speaker_id å‚æ•°
@app.get("/")
async def tts_endpoint(
    refer_wav_path: str = None,
    prompt_text: str = None,
    prompt_language: str = None,
    text: str = None,
    text_language: str = None,
    cut_punc: str = None,
    top_k: int = 15,
    top_p: float = 1.0,
    temperature: float = 1.0,
    speed: float = 1.0,
    inp_refs: list = Query(default=[]),
    sample_steps: int = 32,
    if_sr: bool = False,
    speaker_id: str = "default"  # æ–°å¢ speaker_id å‚æ•°
):
    return handle(
        refer_wav_path,
        prompt_text,
        prompt_language,
        text,
        text_language,
        cut_punc,
        top_k,
        top_p,
        temperature,
        speed,
        inp_refs,
        sample_steps,
        if_sr,
        speaker_id
    )


if __name__ == "__main__":
    import threading
    import time
    import secrets
    
    # 1. å¯åŠ¨ FastAPI æœåŠ¡å™¨çº¿ç¨‹
    def run_server():
        uvicorn.run(app, host=host, port=port, workers=1)
    
    server_thread = threading.Thread(target=run_server, daemon=True)
    server_thread.start()
    print(f"ğŸš€ å¯åŠ¨å†…éƒ¨ FastAPI æœåŠ¡å™¨ (ç«¯å£: {port})...")
    time.sleep(3)  # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
    
    # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦åˆ›å»ºå…¬å¼€é“¾æ¥
    if args.public:
        print("ğŸŒ æ­£åœ¨åˆ›å»ºå…¬å¼€éš§é“é“¾æ¥...")
        print("="*60)
        
        try:
            # ä½¿ç”¨æŒ‡å®šå­åŸŸåæˆ–ç”Ÿæˆéšæœºä»¤ç‰Œ
            share_token = args.sd if args.sd else secrets.token_urlsafe(32)
            
            public_url = setup_tunnel(
                local_host="127.0.0.1",
                local_port=port,
                share_token=share_token,
                share_server_address=None,
            )
            
            print(f"\nâœ… éš§é“åˆ›å»ºæˆåŠŸï¼æ‚¨çš„å…¬å¼€è®¿é—®ä¿¡æ¯ï¼š")
            print("="*60)
            print(f"ğŸ“¢ å…¬å¼€ URL: {public_url}")
            print(f"ğŸ”§ API æ ¹è·¯å¾„: {public_url}/")
            print(f"ğŸ¤ è¯­éŸ³åˆæˆ: {public_url}/voice")
            print(f"ğŸ‘¥ è¯´è¯äººåˆ—è¡¨: {public_url}/voice/speakers")
            print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯: {public_url}/models/info")
            print(f"ğŸ–¥ï¸  ç³»ç»ŸçŠ¶æ€: {public_url}/status")
            print("="*60)
            print("ğŸ’¡ æç¤ºï¼š")
            print("   â€¢ å¯é€šè¿‡GETæˆ–POSTè¯·æ±‚è®¿é—®è¯­éŸ³åˆæˆAPI")
            print("   â€¢ æ­¤é“¾æ¥é»˜è®¤æœ‰æ•ˆæœŸä¸º72å°æ—¶")
            if args.sd:
                print(f"   â€¢ ä½¿ç”¨å›ºå®šå­åŸŸå: {args.sd}")
            else:
                print(f"   â€¢ ä½¿ç”¨éšæœºä»¤ç‰Œ: {share_token[:16]}...")
            print("   â€¢ æŒ‰ Ctrl+C å¯ç»ˆæ­¢æœåŠ¡")
            print("="*60 + "\n")
            
        except requests.exceptions.ConnectionError:
            print(f"\nâš ï¸  ç½‘ç»œé”™è¯¯ï¼šæ— æ³•è¿æ¥åˆ° Gradio éš§é“æœåŠ¡å™¨")
            print(f"   è¿™å¯èƒ½æ˜¯å› ä¸ºç½‘ç»œé™åˆ¶")
            print(f"   âš ï¸  å…¬å¼€é“¾æ¥åˆ›å»ºå¤±è´¥ï¼Œä»…é™æœ¬åœ°è®¿é—®")
            print(f"   æœ¬åœ°è®¿é—®åœ°å€ï¼š")
            print(f"   â€¢ http://localhost:{config.server_config.port}")
            print(f"   â€¢ http://0.0.0.0:{config.server_config.port}")
        except Exception as e:
            print(f"\nâŒ åˆ›å»ºéš§é“æ—¶å‘ç”Ÿé”™è¯¯ï¼š{type(e).__name__}: {e}")
            print(f"   âš ï¸  å…¬å¼€é“¾æ¥åˆ›å»ºå¤±è´¥ï¼Œä»…é™æœ¬åœ°è®¿é—®")
            print(f"   æœ¬åœ°è®¿é—®åœ°å€ï¼šhttp://localhost:{config.server_config.port}")
    else:
        # ä¸åˆ›å»ºå…¬å¼€é“¾æ¥ï¼Œä»…æ˜¾ç¤ºæœ¬åœ°åœ°å€
        print("ğŸ”’ æœªå¯ç”¨å…¬å¼€é“¾æ¥åŠŸèƒ½ï¼ˆå¦‚éœ€å¯ç”¨è¯·æ·»åŠ  --public å‚æ•°ï¼‰")
        print("="*60)
        print(f"ğŸ“¢ æœ¬åœ°è®¿é—®åœ°å€ï¼š")
        print(f"   â€¢ http://localhost:{config.server_config.port}")
        print(f"   â€¢ http://0.0.0.0:{config.server_config.port}")
        if args.sd:
            print(f"   â€¢ æ£€æµ‹åˆ°å­åŸŸåå‚æ•° --sd {args.sd}ï¼Œä½†æœªå¯ç”¨å…¬å¼€é“¾æ¥")
        print("   â€¢ æŒ‰ Ctrl+C å¯ç»ˆæ­¢æœåŠ¡")
        print("="*60 + "\n")
    
    # ä¸»å¾ªç¯ä¿æŒç¨‹åºè¿è¡Œ
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­æœåŠ¡...")


