import argparse
import os
import re
import sys

now_dir = os.getcwd()
sys.path.append(now_dir)
sys.path.append("%s/GPT_SoVITS" % (now_dir))

import signal
from text.LangSegmenter import LangSegmenter
from time import time as ttime
import torch
import torchaudio
import librosa
import soundfile as sf
from fastapi import FastAPI, Request, Query
from fastapi.responses import StreamingResponse, JSONResponse
import uvicorn
from transformers import AutoModelForMaskedLM, AutoTokenizer
import numpy as np
from feature_extractor import cnhubert
from io import BytesIO
from module.models import Generator, SynthesizerTrn, SynthesizerTrnV3
from peft import LoraConfig, get_peft_model
from AR.models.t2s_lightning_module import Text2SemanticLightningModule
from text import cleaned_text_to_sequence
from text.cleaner import clean_text
from module.mel_processing import spectrogram_torch
import config as global_config
import logging
import subprocess

# æ¨¡å‹å®ä¾‹è®¿é—®è®°å½•ï¼Œç”¨äºLRUæ·˜æ±°
model_access_times = {}
# å½“å‰å·²åŠ è½½çš„æ¨¡å‹è®¡æ•°
loaded_models_count = 0
# é•¿æ–‡æœ¬é˜ˆå€¼
LONG_TEXT_THRESHOLD = 70

# --- ä»è¿™é‡Œå¼€å§‹ï¼Œå¤åˆ¶ gradio_tunneling çš„æ ¸å¿ƒä»£ç  (æ”¾åœ¨æ–‡ä»¶é¡¶éƒ¨çš„ import åŒºåŸŸ) ---
import atexit
import platform
import stat
import time
from pathlib import Path
from typing import List, Optional

import requests

VERSION = "0.2"
CURRENT_TUNNELS: List["Tunnel"] = []

machine = platform.machine()
if machine == "x86_64":
    machine = "amd64"

BINARY_REMOTE_NAME = f"frpc_{platform.system().lower()}_{machine.lower()}"
EXTENSION = ".exe" if os.name == "nt" else ""
BINARY_URL = f"https://cdn-media.huggingface.co/frpc-gradio-{VERSION}/{BINARY_REMOTE_NAME}{EXTENSION}"

BINARY_FILENAME = f"{BINARY_REMOTE_NAME}_v{VERSION}"
BINARY_FOLDER = Path(__file__).parent.absolute()
BINARY_PATH = f"{BINARY_FOLDER / BINARY_FILENAME}"

TUNNEL_TIMEOUT_SECONDS = 30
TUNNEL_ERROR_MESSAGE = (
    "Could not create share URL. "
    "Please check the appended log from frpc for more information:"
)

GRADIO_API_SERVER = "https://api.gradio.app/v2/tunnel-request"
GRADIO_SHARE_SERVER_ADDRESS = None


class Tunnel:
    def __init__(self, remote_host, remote_port, local_host, local_port, share_token):
        self.proc = None
        self.url = None
        self.remote_host = remote_host
        self.remote_port = remote_port
        self.local_host = local_host
        self.local_port = local_port
        self.share_token = share_token

    @staticmethod
    def download_binary():
        if not Path(BINARY_PATH).exists():
            resp = requests.get(BINARY_URL)

            if resp.status_code == 403:
                raise OSError(
                    f"Cannot set up a share link as this platform is incompatible. Please "
                    f"create a GitHub issue with information about your platform: {platform.uname()}"
                )

            resp.raise_for_status()

            # Save file data to local copy
            with open(BINARY_PATH, "wb") as file:
                file.write(resp.content)
            st = os.stat(BINARY_PATH)
            os.chmod(BINARY_PATH, st.st_mode | stat.S_IEXEC)

    def start_tunnel(self) -> str:
        self.download_binary()
        self.url = self._start_tunnel(BINARY_PATH)
        return self.url

    def kill(self):
        if self.proc is not None:
            print(f"Killing tunnel {self.local_host}:{self.local_port} <> {self.url}")
            self.proc.terminate()
            self.proc = None

    def _start_tunnel(self, binary: str) -> str:
        CURRENT_TUNNELS.append(self)
        command = [
            binary,
            "http",
            "-n",
            self.share_token,
            "-l",
            str(self.local_port),
            "-i",
            self.local_host,
            "--uc",
            "--sd",
            "random",
            "--ue",
            "--server_addr",
            f"{self.remote_host}:{self.remote_port}",
            "--disable_log_color",
        ]
        self.proc = subprocess.Popen(
            command, stdout=subprocess.PIPE, stderr=subprocess.PIPE
        )
        atexit.register(self.kill)
        return self._read_url_from_tunnel_stream()

    def _read_url_from_tunnel_stream(self) -> str:
        start_timestamp = time.time()

        log = []
        url = ""

        def _raise_tunnel_error():
            log_text = "\n".join(log)
            print(log_text, file=sys.stderr)
            raise ValueError(f"{TUNNEL_ERROR_MESSAGE}\n{log_text}")

        while url == "":
            # check for timeout and log
            if time.time() - start_timestamp >= TUNNEL_TIMEOUT_SECONDS:
                _raise_tunnel_error()

            assert self.proc is not None
            if self.proc.stdout is None:
                continue

            line = self.proc.stdout.readline()
            line = line.decode("utf-8")

            if line == "":
                continue

            log.append(line.strip())

            if "start proxy success" in line:
                result = re.search("start proxy success: (.+)\n", line)
                if result is None:
                    _raise_tunnel_error()
                else:
                    url = result.group(1)
            elif "login to server failed" in line:
                _raise_tunnel_error()

        return url


def setup_tunnel(
    local_host: str,
    local_port: int,
    share_token: str,
    share_server_address: Optional[str],
) -> str:
    share_server_address = (
        GRADIO_SHARE_SERVER_ADDRESS
        if share_server_address is None
        else share_server_address
    )
    if share_server_address is None:
        response = requests.get(GRADIO_API_SERVER)
        if not (response and response.status_code == 200):
            raise RuntimeError("Could not get share link from Gradio API Server.")
        payload = response.json()[0]
        remote_host, remote_port = payload["host"], int(payload["port"])
    else:
        remote_host, remote_port = share_server_address.split(":")
        remote_port = int(remote_port)
    try:
        tunnel = Tunnel(remote_host, remote_port, local_host, local_port, share_token)
        address = tunnel.start_tunnel()
        return address
    except Exception as e:
        raise RuntimeError(str(e)) from e
# --- å¤åˆ¶åˆ°æ­¤ç»“æŸ ---

class DefaultRefer:
    def __init__(self, path, text, language):
        self.path = args.default_refer_path
        self.text = args.default_refer_text
        self.language = args.default_refer_language

    def is_ready(self) -> bool:
        return is_full(self.path, self.text, self.language)


def is_empty(*items):  # ä»»æ„ä¸€é¡¹ä¸ä¸ºç©ºè¿”å›False
    for item in items:
        if item is not None and item != "":
            return False
    return True


def is_full(*items):  # ä»»æ„ä¸€é¡¹ä¸ºç©ºè¿”å›False
    for item in items:
        if item is None or item == "":
            return False
    return True


bigvgan_model = hifigan_model = sv_cn_model = None
def clean_hifigan_model():
    global hifigan_model
    if hifigan_model:
        hifigan_model = hifigan_model.cpu()
        hifigan_model = None
        try:
            torch.cuda.empty_cache()
        except:
            pass
def clean_bigvgan_model():
    global bigvgan_model
    if bigvgan_model:
        bigvgan_model = bigvgan_model.cpu()
        bigvgan_model = None
        try:
            torch.cuda.empty_cache()
        except:
            pass
def clean_sv_cn_model():
    global sv_cn_model
    if sv_cn_model:
        sv_cn_model.embedding_model = sv_cn_model.embedding_model.cpu()
        sv_cn_model = None
        try:
            torch.cuda.empty_cache()
        except:
            pass


def init_bigvgan(target_gpu="cuda:0"):
    # å£°æ˜å…¨å±€å˜é‡
    global bigvgan_model
    
    from BigVGAN import bigvgan
    
    bigvgan_model = bigvgan.BigVGAN.from_pretrained(
        "%s/GPT_SoVITS/pretrained_models/models--nvidia--bigvgan_v2_24khz_100band_256x" % (now_dir,),
        use_cuda_kernel=False,
    )
    bigvgan_model.remove_weight_norm()
    bigvgan_model = bigvgan_model.eval()
    
    if is_half == True:
        bigvgan_model = bigvgan_model.half().to(target_gpu)
    else:
        bigvgan_model = bigvgan_model.to(target_gpu)

def init_hifigan(target_gpu="cuda:0"):
    # å£°æ˜å…¨å±€å˜é‡
    global hifigan_model
    
    hifigan_model = Generator(
        initial_channel=100,
        resblock="1",
        resblock_kernel_sizes=[3, 7, 11],
        resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],
        upsample_rates=[10, 6, 2, 2, 2],
        upsample_initial_channel=512,
        upsample_kernel_sizes=[20, 12, 4, 4, 4],
        gin_channels=0,
        is_bias=True,
    )
    hifigan_model.eval()
    hifigan_model.remove_weight_norm()
    state_dict_g = torch.load(
        "%s/GPT_SoVITS/pretrained_models/gsv-v4-pretrained/vocoder.pth" % (now_dir,),
        map_location="cpu",
        weights_only=False,
    )
    print("loading vocoder", hifigan_model.load_state_dict(state_dict_g))
    if is_half == True:
        hifigan_model = hifigan_model.half().to(target_gpu)
    else:
        hifigan_model = hifigan_model.to(target_gpu)

from sv import SV
def init_sv_cn():
    global hifigan_model
    sv_cn_model = SV(device, is_half)


resample_transform_dict = {}


def resample(audio_tensor, sr0, sr1, device):
    global resample_transform_dict
    key = "%s-%s-%s" % (sr0, sr1, str(device))
    if key not in resample_transform_dict:
        resample_transform_dict[key] = torchaudio.transforms.Resample(sr0, sr1).to(device)
    return resample_transform_dict[key](audio_tensor)


from module.mel_processing import mel_spectrogram_torch

spec_min = -12
spec_max = 2


def norm_spec(x):
    return (x - spec_min) / (spec_max - spec_min) * 2 - 1


def denorm_spec(x):
    return (x + 1) / 2 * (spec_max - spec_min) + spec_min


mel_fn = lambda x: mel_spectrogram_torch(
    x,
    **{
        "n_fft": 1024,
        "win_size": 1024,
        "hop_size": 256,
        "num_mels": 100,
        "sampling_rate": 24000,
        "fmin": 0,
        "fmax": None,
        "center": False,
    },
)
mel_fn_v4 = lambda x: mel_spectrogram_torch(
    x,
    **{
        "n_fft": 1280,
        "win_size": 1280,
        "hop_size": 320,
        "num_mels": 100,
        "sampling_rate": 32000,
        "fmin": 0,
        "fmax": None,
        "center": False,
    },
)


sr_model = None


def audio_sr(audio, sr):
    global sr_model
    if sr_model == None:
        from tools.audio_sr import AP_BWE

        try:
            sr_model = AP_BWE(device, DictToAttrRecursive)
        except FileNotFoundError:
            logger.info("ä½ æ²¡æœ‰ä¸‹è½½è¶…åˆ†æ¨¡å‹çš„å‚æ•°ï¼Œå› æ­¤ä¸è¿›è¡Œè¶…åˆ†ã€‚å¦‚æƒ³è¶…åˆ†è¯·å…ˆå‚ç…§æ•™ç¨‹æŠŠæ–‡ä»¶ä¸‹è½½")
            return audio.cpu().detach().numpy(), sr
    return sr_model(audio, sr)


# 2. ä¿®æ”¹ Speaker ç±»ï¼Œæ·»åŠ  gpt_pathã€sovits_path
class Speaker:
    def __init__(self, name, gpt=None, sovits=None, phones=None, bert=None, prompt=None, gpt_path=None, sovits_path=None, load_time=None,gpu0_gpt=None, gpu0_sovits=None, gpu1_gpt=None, gpu1_sovits=None,
last_used=None):
        self.name = name
        self.gpt = gpt
        self.sovits = sovits
        self.phones = phones
        self.bert = bert
        self.prompt = prompt
        self.gpt_path = gpt_path
        self.sovits_path = sovits_path
        # åŒGPUæ‰©å±•å­—æ®µ
        self.gpu0_gpt = gpu0_gpt
        self.gpu0_sovits = gpu0_sovits
        self.gpu1_gpt = gpu1_gpt
        self.gpu1_sovits = gpu1_sovits
        self.last_used = last_used

class Sovits:
    def __init__(self, vq_model, hps):
        self.vq_model = vq_model
        self.hps = hps


from process_ckpt import get_sovits_version_from_path_fast, load_sovits_new


def get_sovits_weights(sovits_path, target_gpu="cuda:0"):
    from config import pretrained_sovits_name
    path_sovits_v3 = pretrained_sovits_name["v3"]
    path_sovits_v4 = pretrained_sovits_name["v4"]
    is_exist_s2gv3 = os.path.exists(path_sovits_v3)
    is_exist_s2gv4 = os.path.exists(path_sovits_v4)

    version, model_version, if_lora_v3 = get_sovits_version_from_path_fast(sovits_path)
    is_exist = is_exist_s2gv3 if model_version == "v3" else is_exist_s2gv4
    path_sovits = path_sovits_v3 if model_version == "v3" else path_sovits_v4

    if if_lora_v3 == True and is_exist == False:
        logger.info("SoVITS %s åº•æ¨¡ç¼ºå¤±ï¼Œæ— æ³•åŠ è½½ç›¸åº” LoRA æƒé‡" % model_version)

    dict_s2 = load_sovits_new(sovits_path)
    hps = dict_s2["config"]
    hps = DictToAttrRecursive(hps)
    hps.model.semantic_frame_rate = "25hz"
    if "enc_p.text_embedding.weight" not in dict_s2["weight"]:
        hps.model.version = "v2"  # v3model,v2sybomls
    elif dict_s2["weight"]["enc_p.text_embedding.weight"].shape[0] == 322:
        hps.model.version = "v1"
    else:
        hps.model.version = "v2"

    model_params_dict = vars(hps.model)
    if model_version not in {"v3", "v4"}:
        if "Pro" in model_version:
            hps.model.version = model_version
            if sv_cn_model == None:
                init_sv_cn()

        vq_model = SynthesizerTrn(
            hps.data.filter_length // 2 + 1,
            hps.train.segment_size // hps.data.hop_length,
            n_speakers=hps.data.n_speakers,
            **model_params_dict,
        )
    else:
        hps.model.version = model_version
        vq_model = SynthesizerTrnV3(
            hps.data.filter_length // 2 + 1,
            hps.train.segment_size // hps.data.hop_length,
            n_speakers=hps.data.n_speakers,
            **model_params_dict,
        )
        if model_version == "v3":
            init_bigvgan()
        if model_version == "v4":
            init_hifigan()

    model_version = hps.model.version
    logger.info(f"æ¨¡å‹ç‰ˆæœ¬: {model_version}")
    if "pretrained" not in sovits_path:
        try:
            del vq_model.enc_q
        except:
            pass
    if is_half == True:
        vq_model = vq_model.half().to(target_gpu)
    else:
        vq_model = vq_model.to(target_gpu)
    vq_model.eval()
    if if_lora_v3 == False:
        vq_model.load_state_dict(dict_s2["weight"], strict=False)
    else:
        path_sovits = path_sovits_v3 if model_version == "v3" else path_sovits_v4
        vq_model.load_state_dict(load_sovits_new(path_sovits)["weight"], strict=False)
        lora_rank = dict_s2["lora_rank"]
        lora_config = LoraConfig(
            target_modules=["to_k", "to_q", "to_v", "to_out.0"],
            r=lora_rank,
            lora_alpha=lora_rank,
            init_lora_weights=True,
        )
        vq_model.cfm = get_peft_model(vq_model.cfm, lora_config)
        vq_model.load_state_dict(dict_s2["weight"], strict=False)
        vq_model.cfm = vq_model.cfm.merge_and_unload()
        # torch.save(vq_model.state_dict(),"merge_win.pth")
        vq_model.eval()

    sovits = Sovits(vq_model, hps)
    return sovits


class Gpt:
    def __init__(self, max_sec, t2s_model):
        self.max_sec = max_sec
        self.t2s_model = t2s_model


global hz
hz = 50


def get_gpt_weights(gpt_path, target_gpu="cuda:0"):
    dict_s1 = torch.load(gpt_path, map_location="cpu", weights_only=False)
    config = dict_s1["config"]
    max_sec = config["data"]["max_sec"]
    t2s_model = Text2SemanticLightningModule(config, "****", is_train=False)
    t2s_model.load_state_dict(dict_s1["weight"])
    if is_half == True:
        t2s_model = t2s_model.half()
    t2s_model = t2s_model.to(target_gpu)
    t2s_model.eval()
    # total = sum([param.nelement() for param in t2s_model.parameters()])
    # logger.info("Number of parameter: %.2fM" % (total / 1e6))

    gpt = Gpt(max_sec, t2s_model)
    return gpt


# 8. ä¿®æ”¹ change_gpt_sovits_weights
def change_gpt_sovits_weights(gpt_path, sovits_path, speaker_id="default"):
    try:
        speaker_list[speaker_id] = Speaker(
            name=speaker_id,
            gpt=None,
            sovits=None,
            prompt=speaker_list.get(speaker_id, Speaker(name=speaker_id, gpt=None, sovits=None)).prompt or {
                "path": "D.wav",
                "text": "æ­Œæ‰‹ã§ã‚¤ãƒ—ã‚·ãƒ­ãƒ³ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚¹ã‚¿ãƒ¼â€¦ã€å‚·ã¤ãèª°ã‹ã®å¿ƒã‚’å®ˆã‚‹ã“ã¨ãŒã§ããŸãªã‚‰ã€ã£ã¦ã€ã‚¢ãƒŠã‚¿ã®ä½œå“ã ã‚ˆã­ï¼Ÿ",
                "prompt_language": "ja"
            },
            gpt_path=gpt_path,
            sovits_path=sovits_path
        )
        return JSONResponse({"code": 0, "message": "Success"}, status_code=200)
    except Exception as e:
        return JSONResponse({"code": 400, "message": str(e)}, status_code=400)


def get_bert_feature(text, word2ph):
    with torch.no_grad():
        inputs = tokenizer(text, return_tensors="pt")
        for i in inputs:
            inputs[i] = inputs[i].to(device)  #####è¾“å…¥æ˜¯longä¸ç”¨ç®¡ç²¾åº¦é—®é¢˜ï¼Œç²¾åº¦éšbert_model
        res = bert_model(**inputs, output_hidden_states=True)
        res = torch.cat(res["hidden_states"][-3:-2], -1)[0].cpu()[1:-1]
    assert len(word2ph) == len(text)
    phone_level_feature = []
    for i in range(len(word2ph)):
        repeat_feature = res[i].repeat(word2ph[i], 1)
        phone_level_feature.append(repeat_feature)
    phone_level_feature = torch.cat(phone_level_feature, dim=0)
    # if(is_half==True):phone_level_feature=phone_level_feature.half()
    return phone_level_feature.T


def clean_text_inf(text, language, version):
    language = language.replace("all_", "")
    phones, word2ph, norm_text = clean_text(text, language, version)
    phones = cleaned_text_to_sequence(phones, version)
    return phones, word2ph, norm_text


def get_bert_inf(phones, word2ph, norm_text, language):
    language = language.replace("all_", "")
    if language == "zh":
        bert = get_bert_feature(norm_text, word2ph).to(device)  # .to(dtype)
    else:
        bert = torch.zeros(
            (1024, len(phones)),
            dtype=torch.float16 if is_half == True else torch.float32,
        ).to(device)

    return bert


from text import chinese


def get_phones_and_bert(text, language, version, final=False):
    text = re.sub(r' {2,}', ' ', text)
    textlist = []
    langlist = []
    if language == "all_zh":
        for tmp in LangSegmenter.getTexts(text,"zh"):
            langlist.append(tmp["lang"])
            textlist.append(tmp["text"])
    elif language == "all_yue":
        for tmp in LangSegmenter.getTexts(text,"zh"):
            if tmp["lang"] == "zh":
                tmp["lang"] = "yue"
            langlist.append(tmp["lang"])
            textlist.append(tmp["text"])
    elif language == "all_ja":
        for tmp in LangSegmenter.getTexts(text,"ja"):
            langlist.append(tmp["lang"])
            textlist.append(tmp["text"])
    elif language == "all_ko":
        for tmp in LangSegmenter.getTexts(text,"ko"):
            langlist.append(tmp["lang"])
            textlist.append(tmp["text"])
    elif language == "en":
        langlist.append("en")
        textlist.append(text)
    elif language == "auto":
        for tmp in LangSegmenter.getTexts(text):
            langlist.append(tmp["lang"])
            textlist.append(tmp["text"])
    elif language == "auto_yue":
        for tmp in LangSegmenter.getTexts(text):
            if tmp["lang"] == "zh":
                tmp["lang"] = "yue"
            langlist.append(tmp["lang"])
            textlist.append(tmp["text"])
    else:
        for tmp in LangSegmenter.getTexts(text):
            if langlist:
                if (tmp["lang"] == "en" and langlist[-1] == "en") or (tmp["lang"] != "en" and langlist[-1] != "en"):
                    textlist[-1] += tmp["text"]
                    continue
            if tmp["lang"] == "en":
                langlist.append(tmp["lang"])
            else:
                # å› æ— æ³•åŒºåˆ«ä¸­æ—¥éŸ©æ–‡æ±‰å­—,ä»¥ç”¨æˆ·è¾“å…¥ä¸ºå‡†
                langlist.append(language)
            textlist.append(tmp["text"])
    phones_list = []
    bert_list = []
    norm_text_list = []
    for i in range(len(textlist)):
        lang = langlist[i]
        phones, word2ph, norm_text = clean_text_inf(textlist[i], lang, version)
        bert = get_bert_inf(phones, word2ph, norm_text, lang)
        phones_list.append(phones)
        norm_text_list.append(norm_text)
        bert_list.append(bert)
    bert = torch.cat(bert_list, dim=1)
    phones = sum(phones_list, [])
    norm_text = "".join(norm_text_list)

    if not final and len(phones) < 6:
        return get_phones_and_bert("." + text, language, version, final=True)

    return phones, bert.to(torch.float16 if is_half == True else torch.float32), norm_text


class DictToAttrRecursive(dict):
    def __init__(self, input_dict):
        super().__init__(input_dict)
        for key, value in input_dict.items():
            if isinstance(value, dict):
                value = DictToAttrRecursive(value)
            self[key] = value
            setattr(self, key, value)

    def __getattr__(self, item):
        try:
            return self[item]
        except KeyError:
            raise AttributeError(f"Attribute {item} not found")

    def __setattr__(self, key, value):
        if isinstance(value, dict):
            value = DictToAttrRecursive(value)
        super(DictToAttrRecursive, self).__setitem__(key, value)
        super().__setattr__(key, value)

    def __delattr__(self, item):
        try:
            del self[item]
        except KeyError:
            raise AttributeError(f"Attribute {item} not found")


def get_spepc(hps, filename, dtype, device, is_v2pro=False):
    sr1 = int(hps.data.sampling_rate)
    audio, sr0 = torchaudio.load(filename)
    if sr0 != sr1:
        audio = audio.to(device)
        if audio.shape[0] == 2:
            audio = audio.mean(0).unsqueeze(0)
        audio = resample(audio, sr0, sr1, device)
    else:
        audio = audio.to(device)
        if audio.shape[0] == 2:
            audio = audio.mean(0).unsqueeze(0)

    maxx = audio.abs().max()
    if maxx > 1:
        audio /= min(2, maxx)
    spec = spectrogram_torch(
        audio,
        hps.data.filter_length,
        hps.data.sampling_rate,
        hps.data.hop_length,
        hps.data.win_length,
        center=False,
    )
    spec = spec.to(dtype)
    if is_v2pro == True:
        audio = resample(audio, sr1, 16000, device).to(dtype)
    return spec, audio


def pack_audio(audio_bytes, data, rate):
    if media_type == "ogg":
        audio_bytes = pack_ogg(audio_bytes, data, rate)
    elif media_type == "aac":
        audio_bytes = pack_aac(audio_bytes, data, rate)
    else:
        # wavæ— æ³•æµå¼, å…ˆæš‚å­˜raw
        audio_bytes = pack_raw(audio_bytes, data, rate)

    return audio_bytes


def pack_ogg(audio_bytes, data, rate):
    # Author: AkagawaTsurunaki
    # Issue:
    #   Stack overflow probabilistically occurs
    #   when the function `sf_writef_short` of `libsndfile_64bit.dll` is called
    #   using the Python library `soundfile`
    # Note:
    #   This is an issue related to `libsndfile`, not this project itself.
    #   It happens when you generate a large audio tensor (about 499804 frames in my PC)
    #   and try to convert it to an ogg file.
    # Related:
    #   https://github.com/RVC-Boss/GPT-SoVITS/issues/1199
    #   https://github.com/libsndfile/libsndfile/issues/1023
    #   https://github.com/bastibe/python-soundfile/issues/396
    # Suggestion:
    #   Or split the whole audio data into smaller audio segment to avoid stack overflow?

    def handle_pack_ogg():
        with sf.SoundFile(audio_bytes, mode="w", samplerate=rate, channels=1, format="ogg") as audio_file:
            audio_file.write(data)

    import threading

    # See: https://docs.python.org/3/library/threading.html
    # The stack size of this thread is at least 32768
    # If stack overflow error still occurs, just modify the `stack_size`.
    # stack_size = n * 4096, where n should be a positive integer.
    # Here we chose n = 4096.
    stack_size = 4096 * 4096
    try:
        threading.stack_size(stack_size)
        pack_ogg_thread = threading.Thread(target=handle_pack_ogg)
        pack_ogg_thread.start()
        pack_ogg_thread.join()
    except RuntimeError as e:
        # If changing the thread stack size is unsupported, a RuntimeError is raised.
        print("RuntimeError: {}".format(e))
        print("Changing the thread stack size is unsupported.")
    except ValueError as e:
        # If the specified stack size is invalid, a ValueError is raised and the stack size is unmodified.
        print("ValueError: {}".format(e))
        print("The specified stack size is invalid.")

    return audio_bytes


def pack_raw(audio_bytes, data, rate):
    audio_bytes.write(data.tobytes())

    return audio_bytes


def pack_wav(audio_bytes, rate):
    if is_int32:
        data = np.frombuffer(audio_bytes.getvalue(), dtype=np.int32)
        wav_bytes = BytesIO()
        sf.write(wav_bytes, data, rate, format="WAV", subtype="PCM_32")
    else:
        data = np.frombuffer(audio_bytes.getvalue(), dtype=np.int16)
        wav_bytes = BytesIO()
        sf.write(wav_bytes, data, rate, format="WAV")
    return wav_bytes


def pack_aac(audio_bytes, data, rate):
    if is_int32:
        pcm = "s32le"
        bit_rate = "256k"
    else:
        pcm = "s16le"
        bit_rate = "128k"
    process = subprocess.Popen(
        [
            "ffmpeg",
            "-f",
            pcm,  # è¾“å…¥16ä½æœ‰ç¬¦å·å°ç«¯æ•´æ•°PCM
            "-ar",
            str(rate),  # è®¾ç½®é‡‡æ ·ç‡
            "-ac",
            "1",  # å•å£°é“
            "-i",
            "pipe:0",  # ä»ç®¡é“è¯»å–è¾“å…¥
            "-c:a",
            "aac",  # éŸ³é¢‘ç¼–ç å™¨ä¸ºAAC
            "-b:a",
            bit_rate,  # æ¯”ç‰¹ç‡
            "-vn",  # ä¸åŒ…å«è§†é¢‘
            "-f",
            "adts",  # è¾“å‡ºAACæ•°æ®æµæ ¼å¼
            "pipe:1",  # å°†è¾“å‡ºå†™å…¥ç®¡é“
        ],
        stdin=subprocess.PIPE,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
    )
    out, _ = process.communicate(input=data.tobytes())
    audio_bytes.write(out)

    return audio_bytes


def read_clean_buffer(audio_bytes):
    audio_chunk = audio_bytes.getvalue()
    audio_bytes.truncate(0)
    audio_bytes.seek(0)

    return audio_bytes, audio_chunk


def cut_text(text, punc):
    punc_list = [p for p in punc if p in {",", ".", ";", "?", "!", "ã€", "ï¼Œ", "ã€‚", "ï¼Ÿ", "ï¼", "ï¼›", "ï¼š", "â€¦"}]
    if len(punc_list) > 0:
        punds = r"[" + "".join(punc_list) + r"]"
        text = text.strip("\n")
        items = re.split(f"({punds})", text)
        mergeitems = ["".join(group) for group in zip(items[::2], items[1::2])]
        # åœ¨å¥å­ä¸å­˜åœ¨ç¬¦å·æˆ–å¥å°¾æ— ç¬¦å·çš„æ—¶å€™ä¿è¯æ–‡æœ¬å®Œæ•´
        if len(items) % 2 == 1:
            mergeitems.append(items[-1])
        text = "\n".join(mergeitems)

    while "\n\n" in text:
        text = text.replace("\n\n", "\n")

    return text


def only_punc(text):
    return not any(t.isalnum() or t.isalpha() for t in text)


splits = {
    "ï¼Œ",
    "ã€‚",
    "ï¼Ÿ",
    "ï¼",
    ",",
    ".",
    "?",
    "!",
    "~",
    ":",
    "ï¼š",
    "â€”",
    "â€¦",
}


def unload_least_recently_used():
    """å¸è½½æœ€ä¹…æœªä½¿ç”¨çš„æ¨¡å‹ï¼ˆåŒæ—¶å¸è½½è¯¥è¯´è¯äººåœ¨ä¸¤ä¸ªGPUä¸Šçš„å®ä¾‹ï¼‰"""
    # å£°æ˜å…¨å±€å˜é‡
    global loaded_models_count
    
    # æ‰¾åˆ°æœ€ä¹…æœªä½¿ç”¨çš„è¯´è¯äºº
    if not speaker_list:
        return
    
    # æ‰¾åˆ°æœ‰æ¨¡å‹åŠ è½½ä¸”æœ€ä¹…æœªä½¿ç”¨çš„è¯´è¯äºº
    candidates = []
    for speaker_id, speaker in speaker_list.items():
        if speaker.gpu0_gpt is not None or speaker.gpu1_gpt is not None:
            candidates.append((speaker_id, speaker.last_used or 0))
    
    if not candidates:
        return
    
    oldest_speaker_id = min(candidates, key=lambda x: x[1])[0]
    oldest_speaker = speaker_list[oldest_speaker_id]
    
    # åŒæ—¶å¸è½½è¯¥è¯´è¯äººåœ¨ä¸¤ä¸ªGPUä¸Šçš„å®ä¾‹
    if oldest_speaker.gpu0_gpt is not None:
        oldest_speaker.gpu0_gpt = None
        oldest_speaker.gpu0_sovits = None
        loaded_models_count -= 1
    
    if oldest_speaker.gpu1_gpt is not None:
        oldest_speaker.gpu1_gpt = None
        oldest_speaker.gpu1_sovits = None
        loaded_models_count -= 1
    
    # æ¸…ç†GPUç¼“å­˜
    torch.cuda.empty_cache()
    
    logger.info(f"ğŸ”„ å·²å¸è½½æœ€ä¹…æœªä½¿ç”¨çš„è¯´è¯äºº: {oldest_speaker_id}")

def ensure_model_loaded(speaker_id, gpu_index=None):
    """
    ç¡®ä¿æŒ‡å®šè¯´è¯äººçš„æ¨¡å‹åŠ è½½åˆ°æŒ‡å®šçš„GPUä¸Š
    gpu_index: 0æˆ–1ï¼Œä¸ºNoneæ—¶è‡ªåŠ¨é€‰æ‹©
    """
    from time import time as ttime
    
    # å¿…é¡»åœ¨å‡½æ•°é¡¶éƒ¨å£°æ˜æ‰€æœ‰è¦ä¿®æ”¹çš„å…¨å±€å˜é‡
    global loaded_models_count, model_access_times
    
    if speaker_id not in speaker_list:
        raise ValueError(f"Speaker {speaker_id} not found")
    
    speaker = speaker_list[speaker_id]
    speaker.last_used = ttime()
    
    # æ›´æ–°å…¨å±€è®¿é—®è®°å½•ï¼ˆç”¨äºLRUæ·˜æ±°ï¼‰
    model_access_times[speaker_id] = ttime()
    
    # å¦‚æœæŒ‡å®šäº†GPUç´¢å¼•
    if gpu_index is not None:
        target_gpu = f"cuda:{gpu_index}"
        
        # æ£€æŸ¥è¯¥GPUä¸Šçš„æ¨¡å‹æ˜¯å¦å·²åŠ è½½
        if gpu_index == 0:
            if speaker.gpu0_gpt is None:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦å¸è½½æ—§æ¨¡å‹
                if loaded_models_count >= max_models * 2:
                    unload_least_recently_used()
                
                # åŠ è½½æ¨¡å‹
                speaker.gpu0_gpt = get_gpt_weights(speaker.gpt_path, target_gpu)
                speaker.gpu0_sovits = get_sovits_weights(speaker.sovits_path, target_gpu)
                loaded_models_count += 1
        
        elif gpu_index == 1:
            if speaker.gpu1_gpt is None:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦å¸è½½æ—§æ¨¡å‹
                if loaded_models_count >= max_models * 2:
                    unload_least_recently_used()
                
                # åŠ è½½æ¨¡å‹
                speaker.gpu1_gpt = get_gpt_weights(speaker.gpt_path, target_gpu)
                speaker.gpu1_sovits = get_sovits_weights(speaker.sovits_path, target_gpu)
                loaded_models_count += 1
    
    # å¦‚æœæœªæŒ‡å®šGPUï¼Œç¡®ä¿è‡³å°‘ä¸€ä¸ªGPUæœ‰æ¨¡å‹
    else:
        if speaker.gpu0_gpt is None and speaker.gpu1_gpt is None:
            import random
            selected_gpu = random.choice([0, 1])
            ensure_model_loaded(speaker_id, selected_gpu)
# ä¿®æ”¹ get_tts_wav å‡½æ•°ï¼Œæ­£ç¡®å¤„ç† get_spepc çš„è¿”å›å€¼
def get_tts_wav(
    refer_wav_path,
    prompt_text,
    prompt_language,
    text,
    text_language,
    top_k=15,
    top_p=0.6,
    temperature=0.6,
    speed=1.0,
    inp_refs=None,
    sample_steps=32,
    if_sr=False,
    spk="default"
):
    # 1. è·å–æ¨¡å‹ï¼ˆæ¢å¤åŸå§‹é€»è¾‘ï¼Œåªåœ¨éœ€è¦æ—¶åŠ è½½æ¨¡å‹ï¼‰
    if spk not in speaker_list:
        return JSONResponse({"code": 400, "message": f"speaker_id: {spk} not found"}, status_code=400)
    
    speaker = speaker_list[spk]
    
    # æ›´æ–°æœ€åä½¿ç”¨æ—¶é—´
    from time import time as ttime
    speaker.last_used = ttime()
    
    # 2. ç¡®ä¿æ¨¡å‹å·²åŠ è½½ï¼ˆä½¿ç”¨åŸæœ‰é€»è¾‘ï¼‰
    if speaker.gpt is None or speaker.sovits is None:
        # åŠ è½½æ¨¡å‹åˆ°é»˜è®¤GPU
        speaker.gpt = get_gpt_weights(speaker.gpt_path, device)
        speaker.sovits = get_sovits_weights(speaker.sovits_path, device)
    
    infer_sovits = speaker.sovits
    infer_gpt = speaker.gpt
    
    vq_model = infer_sovits.vq_model
    hps = infer_sovits.hps
    version = vq_model.version
    
    t2s_model = infer_gpt.t2s_model
    max_sec = infer_gpt.max_sec
    
    # 3. å‚æ•°è°ƒæ•´ï¼ˆæ¢å¤åŸå§‹é€»è¾‘ï¼‰
    if version == "v3":
        if sample_steps not in [4, 8, 16, 32, 64, 128]:
            sample_steps = 32
    elif version == "v4":
        if sample_steps not in [4, 8, 16, 32]:
            sample_steps = 8
    
    if if_sr and version != "v3":
        if_sr = False
    
    # 4. å‡†å¤‡å‚è€ƒéŸ³é¢‘ï¼ˆæ¢å¤åŸå§‹é€»è¾‘ï¼‰
    prompt_text = prompt_text.strip("\n")
    if prompt_text[-1] not in splits:
        prompt_text += "ã€‚" if prompt_language != "en" else "."
    
    prompt_language, text = prompt_language, text.strip("\n")
    dtype = torch.float16 if is_half == True else torch.float32
    
    # 5. å¤„ç†å‚è€ƒéŸ³é¢‘ï¼ˆæ¢å¤åŸå§‹é€»è¾‘ï¼‰
    with torch.no_grad():
        wav16k, sr = librosa.load(refer_wav_path, sr=16000)
        wav16k = torch.from_numpy(wav16k)
        
        zero_wav = np.zeros(int(hps.data.sampling_rate * 0.3), dtype=np.float16 if is_half == True else np.float32)
        zero_wav_torch = torch.from_numpy(zero_wav)
        
        if is_half == True:
            wav16k = wav16k.half().to(device)
            zero_wav_torch = zero_wav_torch.half().to(device)
        else:
            wav16k = wav16k.to(device)
            zero_wav_torch = zero_wav_torch.to(device)
        
        wav16k = torch.cat([wav16k, zero_wav_torch])
        
        ssl_content = ssl_model.model(wav16k.unsqueeze(0))["last_hidden_state"].transpose(1, 2)
        codes = vq_model.extract_latent(ssl_content)
        prompt_semantic = codes[0, 0]
        prompt = prompt_semantic.unsqueeze(0).to(device)
    
    # 6. æ–‡æœ¬å¤„ç†ï¼ˆæ¢å¤åŸå§‹é€»è¾‘ï¼‰
    texts = text.split("\n")
    audio_opt = []
    
    for text in texts:
        if only_punc(text):
            continue
        
        if text[-1] not in splits:
            text += "ã€‚" if text_language != "en" else "."
        
        # è·å–éŸ³ç´ å’ŒBERTç‰¹å¾
        phones1, bert1, norm_text1 = get_phones_and_bert(prompt_text, prompt_language, version)
        phones2, bert2, norm_text2 = get_phones_and_bert(text, text_language, version)
        
        bert = torch.cat([bert1, bert2], 1)
        bert = bert.to(device).unsqueeze(0)
        
        all_phoneme_ids = torch.LongTensor(phones1 + phones2).to(device).unsqueeze(0)
        all_phoneme_len = torch.tensor([all_phoneme_ids.shape[-1]]).to(device)
        
        # 7. GPTæ¨ç†
        with torch.no_grad():
            pred_semantic, idx = t2s_model.model.infer_panel(
                all_phoneme_ids,
                all_phoneme_len,
                prompt,
                bert,
                top_k=top_k,
                top_p=top_p,
                temperature=temperature,
                early_stop_num=hz * max_sec,
            )
            pred_semantic = pred_semantic[:, -idx:].unsqueeze(0)
        
        # 8. SoVITSè§£ç 
        if version not in {"v3", "v4"}:
            # v1/v2 ç‰ˆæœ¬
            refer_spec = get_spepc(hps, refer_wav_path, dtype, device)
            audio = (
                vq_model.decode(
                    pred_semantic,
                    torch.LongTensor(phones2).to(device).unsqueeze(0),
                    refer_spec,
                    speed=speed
                )
                .detach()
                .cpu()
                .numpy()[0, 0]
            )
        else:
            # v3/v4 ç‰ˆæœ¬
            phoneme_ids0 = torch.LongTensor(phones1).to(device).unsqueeze(0)
            phoneme_ids1 = torch.LongTensor(phones2).to(device).unsqueeze(0)
            
            refer_spec, _ = get_spepc(hps, refer_wav_path, dtype, device)
            fea_ref, ge = vq_model.decode_encp(prompt.unsqueeze(0), phoneme_ids0, refer_spec)
            
            # åŠ è½½å‚è€ƒéŸ³é¢‘
            ref_audio, sr = torchaudio.load(refer_wav_path)
            ref_audio = ref_audio.to(device).float()
            if ref_audio.shape[0] == 2:
                ref_audio = ref_audio.mean(0).unsqueeze(0)
            
            # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨æ¨¡å‹é…ç½®çš„é‡‡æ ·ç‡ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç çš„
            tgt_sr = hps.data.sampling_rate
            if sr != tgt_sr:
                ref_audio = resample(ref_audio, sr, tgt_sr, device)
            
            # æ ¹æ®ç‰ˆæœ¬é€‰æ‹©melå‡½æ•°
            if version == "v3":
                mel_fn_current = mel_fn
            else:  # v4
                mel_fn_current = mel_fn_v4
            
            mel2 = mel_fn_current(ref_audio)
            mel2 = norm_spec(mel2)
            
            # åˆ†å—è§£ç è¿‡ç¨‹ï¼ˆä¿æŒåŸå§‹é€»è¾‘ï¼‰
            T_min = min(mel2.shape[2], fea_ref.shape[2])
            mel2 = mel2[:, :, :T_min]
            fea_ref = fea_ref[:, :, :T_min]
            Tref = 468 if version == "v3" else 500
            Tchunk = 934 if version == "v3" else 1000
            if T_min > Tref:
                mel2 = mel2[:, :, -Tref:]
                fea_ref = fea_ref[:, :, -Tref:]
                T_min = Tref

            chunk_len = Tchunk - T_min
            mel2 = mel2.to(dtype)
            fea_todo, ge = vq_model.decode_encp(pred_semantic, phoneme_ids1, refer_spec, ge, speed)

            cfm_resss = []
            idx = 0
            while 1:
                fea_todo_chunk = fea_todo[:, :, idx: idx + chunk_len]
                if fea_todo_chunk.shape[-1] == 0:
                    break
                idx += chunk_len
                fea = torch.cat([fea_ref, fea_todo_chunk], 2).transpose(2, 1)
                cfm_res = vq_model.cfm.inference(
                    fea, torch.LongTensor([fea.size(1)]).to(device), mel2, sample_steps, inference_cfg_rate=0
                )
                cfm_res = cfm_res[:, :, mel2.shape[2]:]
                mel2 = cfm_res[:, :, -T_min:]
                fea_ref = fea_todo_chunk[:, :, -T_min:]
                cfm_resss.append(cfm_res)

            cfm_res = torch.cat(cfm_resss, 2)
            cfm_res = denorm_spec(cfm_res)

            # æ ¹æ®ç‰ˆæœ¬é€‰æ‹©å£°ç å™¨
            if version == "v3":
                if bigvgan_model is None:
                    init_bigvgan(device)
                vocoder_model = bigvgan_model
            else:  # v4
                if hifigan_model is None:
                    init_hifigan(device)
                vocoder_model = hifigan_model

            # ç”ŸæˆéŸ³é¢‘
            with torch.inference_mode():
                wav_gen = vocoder_model(cfm_res)
                audio = wav_gen[0][0].cpu().detach().numpy()
        
        audio_opt.append(audio)
        audio_opt.append(zero_wav)
    
    # 9. æ‹¼æ¥éŸ³é¢‘
    audio = np.concatenate(audio_opt)
    audio = librosa.resample(audio, orig_sr=hps.data.sampling_rate, target_sr=32000)
    max_audio = np.abs(audio).max()
    if max_audio > 1:
        audio = audio / max_audio
    
    # 10. è¶…åˆ†è¾¨ç‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if if_sr and sr_model is not None:
        audio, _ = audio_sr(torch.FloatTensor(audio).unsqueeze(0).to(device), hps.data.sampling_rate)
    
    # 11. åŒ…è£…éŸ³é¢‘ä¸ºå­—èŠ‚æµï¼ˆæ¢å¤åŸå§‹é€»è¾‘ï¼‰
    all_audio_bytes = BytesIO()
    if is_int32:
        audio_bytes = pack_audio(all_audio_bytes, (audio * 2147483647).astype(np.int32), 32000)
    else:
        audio_bytes = pack_audio(all_audio_bytes, (audio * 32768).astype(np.int16), 32000)
    
    if media_type == "wav":
        audio_bytes = pack_wav(audio_bytes, 32000)
    
    if stream_mode == "normal":
        audio_bytes, audio_chunk = read_clean_buffer(audio_bytes)
        yield audio_chunk
    else:
        yield audio_bytes.getvalue()


def get_spepc_for_gpu(hps, filename, dtype, target_gpu):
    """
    ä¸ºæŒ‡å®šGPUè·å–é¢‘è°±ï¼ˆä¿®æ”¹è‡ªåŸget_spepcå‡½æ•°ï¼‰
    """
    sr1 = int(hps.data.sampling_rate)
    audio, sr0 = torchaudio.load(filename)
    
    # ç¡®ä¿éŸ³é¢‘åœ¨ç›®æ ‡GPUä¸Š
    audio = audio.to(target_gpu)
    
    if sr0 != sr1:
        if audio.shape[0] == 2:
            audio = audio.mean(0).unsqueeze(0)
        audio = resample(audio, sr0, sr1, target_gpu)
    else:
        if audio.shape[0] == 2:
            audio = audio.mean(0).unsqueeze(0)
    
    maxx = audio.abs().max()
    if maxx > 1:
        audio /= min(2, maxx)
    
    spec = spectrogram_torch(
        audio,
        hps.data.filter_length,
        hps.data.sampling_rate,
        hps.data.hop_length,
        hps.data.win_length,
        center=False,
    )
    spec = spec.to(dtype)
    
    return spec, audio


def handle_control(command):
    if command == "restart":
        os.execl(g_config.python_exec, g_config.python_exec, *sys.argv)
    elif command == "exit":
        os.kill(os.getpid(), signal.SIGTERM)
        exit(0)


def handle_change(path, text, language):
    if is_empty(path, text, language):
        return JSONResponse(
            {"code": 400, "message": 'ç¼ºå°‘ä»»æ„ä¸€é¡¹ä»¥ä¸‹å‚æ•°: "path", "text", "language"'}, status_code=400
        )

    if path != "" or path is not None:
        default_refer.path = path
    if text != "" or text is not None:
        default_refer.text = text
    if language != "" or language is not None:
        default_refer.language = language

    logger.info(f"å½“å‰é»˜è®¤å‚è€ƒéŸ³é¢‘è·¯å¾„: {default_refer.path}")
    logger.info(f"å½“å‰é»˜è®¤å‚è€ƒéŸ³é¢‘æ–‡æœ¬: {default_refer.text}")
    logger.info(f"å½“å‰é»˜è®¤å‚è€ƒéŸ³é¢‘è¯­ç§: {default_refer.language}")
    logger.info(f"is_ready: {default_refer.is_ready()}")

    return JSONResponse({"code": 0, "message": "Success"}, status_code=200)


def unload_model(speaker_id):
    if speaker_id in speaker_list and speaker_list[speaker_id].gpt is not None:
        speaker_list[speaker_id].gpt = None
        speaker_list[speaker_id].sovits = None
        torch.cuda.empty_cache()


def get_speaker_gpt_model(speaker_id, gpu_index=0):
    """è·å–æŒ‡å®šè¯´è¯äººåœ¨æŒ‡å®šGPUä¸Šçš„GPTæ¨¡å‹"""
    speaker = speaker_list[speaker_id]
    if gpu_index == 0:
        return speaker.gpu0_gpt or speaker.gpt  # å›é€€åˆ°å…¼å®¹å­—æ®µ
    elif gpu_index == 1:
        return speaker.gpu1_gpt
    else:
        raise ValueError(f"Invalid GPU index: {gpu_index}")

def get_speaker_sovits_model(speaker_id, gpu_index=0):
    """è·å–æŒ‡å®šè¯´è¯äººåœ¨æŒ‡å®šGPUä¸Šçš„Sovitsæ¨¡å‹"""
    speaker = speaker_list[speaker_id]
    if gpu_index == 0:
        return speaker.gpu0_sovits or speaker.sovits  # å›é€€åˆ°å…¼å®¹å­—æ®µ
    elif gpu_index == 1:
        return speaker.gpu1_sovits
    else:
        raise ValueError(f"Invalid GPU index: {gpu_index}")
    
def split_long_text(text, threshold=LONG_TEXT_THRESHOLD):
    """
    æ™ºèƒ½æ‹†åˆ†é•¿æ–‡æœ¬ï¼Œå°½é‡åœ¨è‡ªç„¶åœé¡¿å¤„æ‹†åˆ†
    è¿”å›ï¼š(is_long, segments)
    - is_long: æ˜¯å¦ä¸ºé•¿æ–‡æœ¬
    - segments: æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨ï¼Œé•¿æ–‡æœ¬æ—¶ä¸º2æ®µï¼ŒçŸ­æ–‡æœ¬æ—¶ä¸º1æ®µ
    """
    if len(text) <= threshold:
        return False, [text]
    
    # å¯»æ‰¾æœ€ä½³çš„æ‹†åˆ†ç‚¹ï¼ˆåœ¨å¥å·ã€é—®å·ã€æ„Ÿå¹å·ã€é€—å·ç­‰ä½ç½®ï¼‰
    split_positions = []
    for i in range(len(text)):
        if i > threshold * 0.3 and i < len(text) - threshold * 0.3:
            if text[i] in 'ã€‚ï¼ï¼Ÿ.!?ï¼›;ï¼Œ,':
                split_positions.append(i)
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„æ ‡ç‚¹ï¼Œåœ¨é˜ˆå€¼ä½ç½®å¼ºåˆ¶æ‹†åˆ†
    if not split_positions:
        split_pos = min(threshold, len(text) - 1)
    else:
        # é€‰æ‹©æœ€æ¥è¿‘ä¸­é—´ä½ç½®çš„æ ‡ç‚¹
        mid_point = len(text) // 2
        split_pos = min(split_positions, key=lambda x: abs(x - mid_point))
    
    # ç¡®ä¿æ‹†åˆ†ç‚¹ä¸æ˜¯æœ€åä¸€ä¸ªå­—ç¬¦
    split_pos = min(split_pos, len(text) - 5)
    
    return True, [text[:split_pos+1], text[split_pos+1:]]

# 6. ä¿®æ”¹ handle å‡½æ•°ï¼Œè°ƒæ•´ prompt å­—æ®µè®¿é—®
def handle(
    refer_wav_path,
    prompt_text,
    prompt_language,
    text,
    text_language,
    cut_punc,
    top_k,
    top_p,
    temperature,
    speed,
    inp_refs,
    sample_steps,
    if_sr,
    speaker_id="default"
):
    if speaker_id not in speaker_list:
        return JSONResponse({"code": 400, "message": f"speaker_id: {speaker_id} not found"}, status_code=400)

    # ä½¿ç”¨ speaker_list ä¸­å®šä¹‰çš„é»˜è®¤å€¼
    if (
        refer_wav_path == "" or refer_wav_path is None
        or prompt_text == "" or prompt_text is None
        or prompt_language == "" or prompt_language is None
    ):
        refer_wav_path = speaker_list[speaker_id].prompt["ref_audio"] if refer_wav_path in ["", None] else refer_wav_path
        prompt_text = speaker_list[speaker_id].prompt["prompt_text"] if prompt_text in ["", None] else prompt_text
        prompt_language = speaker_list[speaker_id].prompt["prompt_lang"] if prompt_language in ["", None] else prompt_language

        # å¦‚æœä»ç„¶ç¼ºå°‘å¿…è¦å‚æ•°ï¼Œå°è¯•ä½¿ç”¨å…¨å±€é»˜è®¤å‚è€ƒéŸ³é¢‘
        if not is_full(refer_wav_path, prompt_text, prompt_language):
            refer_wav_path = default_refer.path if refer_wav_path in ["", None] else refer_wav_path
            prompt_text = default_refer.text if prompt_text in ["", None] else prompt_text
            prompt_language = default_refer.language if prompt_language in ["", None] else prompt_language
            if not default_refer.is_ready():
                return JSONResponse({"code": 400, "message": "æœªæŒ‡å®šå‚è€ƒéŸ³é¢‘ä¸”æ¥å£æ— é¢„è®¾"}, status_code=400)

    if sample_steps not in [4, 8, 16, 32]:
        sample_steps = 32

    if cut_punc is None:
        text = cut_text(text, default_cut_punc)
    else:
        text = cut_text(text, cut_punc)

    # éªŒè¯ prompt_language å’Œ text_language
    prompt_language = dict_language.get(prompt_language.lower(), prompt_language)
    text_language = dict_language.get(text_language.lower(), text_language)
    supported_languages = ["all_zh", "all_yue", "en", "all_ja", "all_ko", "zh", "yue", "ja", "ko", "auto", "auto_yue"]
    if prompt_language not in supported_languages:
        return JSONResponse({"code": 400, "message": f"prompt_language: {prompt_language} is not supported"}, status_code=400)
    if text_language not in supported_languages:
        return JSONResponse({"code": 400, "message": f"text_language: {text_language} is not supported"}, status_code=400)

    return StreamingResponse(
        get_tts_wav(
            refer_wav_path,
            prompt_text,
            prompt_language,
            text,
            text_language,
            top_k,
            top_p,
            temperature,
            speed,
            inp_refs,
            sample_steps,
            if_sr,
            spk=speaker_id
        ),
        media_type="audio/" + media_type,
    )

# --------------------------------
# åˆå§‹åŒ–éƒ¨åˆ†
# --------------------------------
dict_language = {
    "ä¸­æ–‡": "all_zh",
    "ç²¤è¯­": "all_yue",
    "è‹±æ–‡": "en",
    "æ—¥æ–‡": "all_ja",
    "éŸ©æ–‡": "all_ko",
    "ä¸­è‹±æ··åˆ": "zh",
    "ç²¤è‹±æ··åˆ": "yue",
    "æ—¥è‹±æ··åˆ": "ja",
    "éŸ©è‹±æ··åˆ": "ko",
    "å¤šè¯­ç§æ··åˆ": "auto",  # å¤šè¯­ç§å¯åŠ¨åˆ‡åˆ†è¯†åˆ«è¯­ç§
    "å¤šè¯­ç§æ··åˆ(ç²¤è¯­)": "auto_yue",
    "all_zh": "all_zh",
    "all_yue": "all_yue",
    "en": "en",
    "all_ja": "all_ja",
    "all_ko": "all_ko",
    "zh": "zh",
    "yue": "yue",
    "ja": "ja",
    "ko": "ko",
    "auto": "auto",
    "auto_yue": "auto_yue",
}

# logger
logging.config.dictConfig(uvicorn.config.LOGGING_CONFIG)
logger = logging.getLogger("uvicorn")

# è·å–é…ç½®
g_config = global_config.Config()

# è·å–å‚æ•°
parser = argparse.ArgumentParser(description="GPT-SoVITS api")

parser.add_argument("-s", "--sovits_path", type=str, default=g_config.sovits_path, help="SoVITSæ¨¡å‹è·¯å¾„")
parser.add_argument("-g", "--gpt_path", type=str, default=g_config.gpt_path, help="GPTæ¨¡å‹è·¯å¾„")
parser.add_argument("-dr", "--default_refer_path", type=str, default="", help="é»˜è®¤å‚è€ƒéŸ³é¢‘è·¯å¾„")
parser.add_argument("-dt", "--default_refer_text", type=str, default="", help="é»˜è®¤å‚è€ƒéŸ³é¢‘æ–‡æœ¬")
parser.add_argument("-dl", "--default_refer_language", type=str, default="", help="é»˜è®¤å‚è€ƒéŸ³é¢‘è¯­ç§")
parser.add_argument("-d", "--device", type=str, default=g_config.infer_device, help="cuda / cpu")
parser.add_argument("-a", "--bind_addr", type=str, default="0.0.0.0", help="default: 0.0.0.0")
parser.add_argument("-p", "--port", type=int, default=g_config.api_port, help="default: 9880")
parser.add_argument(
    "-fp", "--full_precision", action="store_true", default=False, help="è¦†ç›–config.is_halfä¸ºFalse, ä½¿ç”¨å…¨ç²¾åº¦"
)
parser.add_argument(
    "-hp", "--half_precision", action="store_true", default=False, help="è¦†ç›–config.is_halfä¸ºTrue, ä½¿ç”¨åŠç²¾åº¦"
)
# boolå€¼çš„ç”¨æ³•ä¸º `python ./api.py -fp ...`
# æ­¤æ—¶ full_precision==True, half_precision==False
parser.add_argument("-sm", "--stream_mode", type=str, default="close", help="æµå¼è¿”å›æ¨¡å¼, close / normal / keepalive")
parser.add_argument("-mt", "--media_type", type=str, default="wav", help="éŸ³é¢‘ç¼–ç æ ¼å¼, wav / ogg / aac")
parser.add_argument("-st", "--sub_type", type=str, default="int16", help="éŸ³é¢‘æ•°æ®ç±»å‹, int16 / int32")
parser.add_argument("-cp", "--cut_punc", type=str, default="", help="æ–‡æœ¬åˆ‡åˆ†ç¬¦å·è®¾å®š, ç¬¦å·èŒƒå›´,.;?!ã€ï¼Œã€‚ï¼Ÿï¼ï¼›ï¼šâ€¦")
# åˆ‡å‰²å¸¸ç”¨åˆ†å¥ç¬¦ä¸º `python ./api.py -cp ".?!ã€‚ï¼Ÿï¼"`
parser.add_argument("-hb", "--hubert_path", type=str, default=g_config.cnhubert_path, help="è¦†ç›–config.cnhubert_path")
parser.add_argument("-b", "--bert_path", type=str, default=g_config.bert_path, help="è¦†ç›–config.bert_path")
parser.add_argument("-mm", "--max_models", type=int, default=3, help="æœ€å¤§åŒæ—¶åŠ è½½æ¨¡å‹æ•°é‡")
# æ·»åŠ ä¸‹é¢è¿™è¡Œæ¥æ”¯æŒè‡ªå®šä¹‰å­åŸŸå
parser.add_argument("--sd", "--subdomain", type=str, default=None, help="æŒ‡å®šéš§é“ä½¿ç”¨çš„å›ºå®šå­åŸŸå (ä¾‹å¦‚: your-name)")

args = parser.parse_args()
sovits_path = args.sovits_path
gpt_path = args.gpt_path
device = args.device
port = args.port
host = args.bind_addr
cnhubert_base_path = args.hubert_path
bert_path = args.bert_path
default_cut_punc = args.cut_punc
max_models = args.max_models

# åº”ç”¨å‚æ•°é…ç½®
default_refer = DefaultRefer(args.default_refer_path, args.default_refer_text, args.default_refer_language)

# æ¨¡å‹è·¯å¾„æ£€æŸ¥
if sovits_path == "":
    sovits_path = g_config.pretrained_sovits_path
    logger.warning(f"æœªæŒ‡å®šSoVITSæ¨¡å‹è·¯å¾„, fallbackåå½“å‰å€¼: {sovits_path}")
if gpt_path == "":
    gpt_path = g_config.pretrained_gpt_path
    logger.warning(f"æœªæŒ‡å®šGPTæ¨¡å‹è·¯å¾„, fallbackåå½“å‰å€¼: {gpt_path}")

# æŒ‡å®šé»˜è®¤å‚è€ƒéŸ³é¢‘, è°ƒç”¨æ–¹ æœªæä¾›/æœªç»™å…¨ å‚è€ƒéŸ³é¢‘å‚æ•°æ—¶ä½¿ç”¨
if default_refer.path == "" or default_refer.text == "" or default_refer.language == "":
    default_refer.path, default_refer.text, default_refer.language = "", "", ""
    logger.info("æœªæŒ‡å®šé»˜è®¤å‚è€ƒéŸ³é¢‘")
else:
    logger.info(f"é»˜è®¤å‚è€ƒéŸ³é¢‘è·¯å¾„: {default_refer.path}")
    logger.info(f"é»˜è®¤å‚è€ƒéŸ³é¢‘æ–‡æœ¬: {default_refer.text}")
    logger.info(f"é»˜è®¤å‚è€ƒéŸ³é¢‘è¯­ç§: {default_refer.language}")

# è·å–åŠç²¾åº¦
is_half = g_config.is_half
if args.full_precision:
    is_half = False
if args.half_precision:
    is_half = True
if args.full_precision and args.half_precision:
    is_half = g_config.is_half  # ç‚’é¥­fallback
logger.info(f"åŠç²¾: {is_half}")

# æµå¼è¿”å›æ¨¡å¼
if args.stream_mode.lower() in ["normal", "n"]:
    stream_mode = "normal"
    logger.info("æµå¼è¿”å›å·²å¼€å¯")
else:
    stream_mode = "close"

# éŸ³é¢‘ç¼–ç æ ¼å¼
if args.media_type.lower() in ["aac", "ogg"]:
    media_type = args.media_type.lower()
elif stream_mode == "close":
    media_type = "wav"
else:
    media_type = "ogg"
logger.info(f"ç¼–ç æ ¼å¼: {media_type}")

# éŸ³é¢‘æ•°æ®ç±»å‹
if args.sub_type.lower() == "int32":
    is_int32 = True
    logger.info("æ•°æ®ç±»å‹: int32")
else:
    is_int32 = False
    logger.info("æ•°æ®ç±»å‹: int16")

# åˆå§‹åŒ–æ¨¡å‹
cnhubert.cnhubert_base_path = cnhubert_base_path
tokenizer = AutoTokenizer.from_pretrained(bert_path)
bert_model = AutoModelForMaskedLM.from_pretrained(bert_path)
ssl_model = cnhubert.get_model()
if is_half:
    bert_model = bert_model.half().to(device)
    ssl_model = ssl_model.half().to(device)
else:
    bert_model = bert_model.to(device)
    ssl_model = ssl_model.to(device)
change_gpt_sovits_weights(gpt_path=gpt_path, sovits_path=sovits_path)


# 1. ä¿®æ”¹ speaker_list åˆå§‹åŒ–ï¼Œæ·»åŠ é»˜è®¤ ref_audioã€prompt_text å’Œ prompt_lang
#n_speaker#S
speaker_list = {}
#n_speaker#E

# --------------------------------
# æ¥å£éƒ¨åˆ†
# --------------------------------
app = FastAPI()

# åœ¨æ¥å£éƒ¨åˆ†æ·»åŠ  /voice/speakers æ¥å£
@app.get("/voice/speakers")
async def get_speakers():
    # åªè¿”å›é€»è¾‘è¯´è¯äººåˆ—è¡¨ï¼Œä¸æš´éœ²å†…éƒ¨çš„åŒGPUç»“æ„
    speakers = [{"name": speaker_id} for speaker_id in speaker_list.keys()]
    return JSONResponse({"GPT-SOVITS": speakers}, status_code=200)

# 6. ä¿®æ”¹ set_model æ¥å£ï¼Œæ”¯æŒ speaker_id
@app.post("/set_model")
async def set_model(request: Request):
    json_post_raw = await request.json()
    return change_gpt_sovits_weights(
        gpt_path=json_post_raw.get("gpt_model_path"),
        sovits_path=json_post_raw.get("sovits_model_path"),
        speaker_id=json_post_raw.get("speaker_id", "default")
    )

@app.get("/set_model")
async def set_model(
    gpt_model_path: str = None,
    sovits_model_path: str = None,
    speaker_id: str = "default"
):
    return change_gpt_sovits_weights(gpt_path=gpt_model_path, sovits_path=sovits_model_path, speaker_id=speaker_id)

@app.post("/control")
async def control(request: Request):
    json_post_raw = await request.json()
    return handle_control(json_post_raw.get("command"))


@app.get("/control")
async def control(command: str = None):
    return handle_control(command)


@app.post("/change_refer")
async def change_refer(request: Request):
    json_post_raw = await request.json()
    return handle_change(
        json_post_raw.get("refer_wav_path"), json_post_raw.get("prompt_text"), json_post_raw.get("prompt_language")
    )


@app.get("/change_refer")
async def change_refer(refer_wav_path: str = None, prompt_text: str = None, prompt_language: str = None):
    return handle_change(refer_wav_path, prompt_text, prompt_language)


# 4. ä¿®æ”¹ tts_endpoint POST æ¥å£ï¼Œæ”¯æŒ speaker_id
@app.post("/")
async def tts_endpoint(request: Request):
    json_post_raw = await request.json()
    return handle(
        json_post_raw.get("refer_wav_path"),
        json_post_raw.get("prompt_text"),
        json_post_raw.get("prompt_language"),
        json_post_raw.get("text"),
        json_post_raw.get("text_language"),
        json_post_raw.get("cut_punc"),
        json_post_raw.get("top_k", 15),
        json_post_raw.get("top_p", 1.0),
        json_post_raw.get("temperature", 1.0),
        json_post_raw.get("speed", 1.0),
        json_post_raw.get("inp_refs", []),
        json_post_raw.get("sample_steps", 32),
        json_post_raw.get("if_sr", False),
        json_post_raw.get("speaker_id", "default")
    )


# 3. ä¿®æ”¹ tts_endpoint GET æ¥å£ï¼Œæ·»åŠ  speaker_id å‚æ•°
@app.get("/")
async def tts_endpoint(
    refer_wav_path: str = None,
    prompt_text: str = None,
    prompt_language: str = None,
    text: str = None,
    text_language: str = None,
    cut_punc: str = None,
    top_k: int = 15,
    top_p: float = 1.0,
    temperature: float = 1.0,
    speed: float = 1.0,
    inp_refs: list = Query(default=[]),
    sample_steps: int = 32,
    if_sr: bool = False,
    speaker_id: str = "default"  # æ–°å¢ speaker_id å‚æ•°
):
    return handle(
        refer_wav_path,
        prompt_text,
        prompt_language,
        text,
        text_language,
        cut_punc,
        top_k,
        top_p,
        temperature,
        speed,
        inp_refs,
        sample_steps,
        if_sr,
        speaker_id
    )


if __name__ == "__main__":
    import threading
    import time
    import secrets
    
    # 1. å¯åŠ¨ FastAPI æœåŠ¡å™¨çº¿ç¨‹
    def run_server():
        uvicorn.run(app, host=host, port=port, workers=1)
    
    server_thread = threading.Thread(target=run_server, daemon=True)
    server_thread.start()
    print(f"ğŸš€ å¯åŠ¨å†…éƒ¨ FastAPI æœåŠ¡å™¨ (ç«¯å£: {port})...")
    time.sleep(3)  # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
    
    # 2. åˆ›å»ºå…¬å¼€éš§é“ï¼ˆæ”¯æŒè‡ªå®šä¹‰å­åŸŸåï¼‰
    print("ğŸŒ æ­£åœ¨åˆ›å»ºå…¬å¼€éš§é“é“¾æ¥...")
    try:
        # å¦‚æœç”¨æˆ·é€šè¿‡ --sd æŒ‡å®šäº†å­åŸŸåï¼Œå°±ä½¿ç”¨å®ƒï¼Œå¦åˆ™ç”Ÿæˆéšæœºä»¤ç‰Œ
        share_token = args.sd if args.sd else secrets.token_urlsafe(32)
        
        public_url = setup_tunnel(
            local_host="127.0.0.1",
            local_port=port,
            share_token=share_token,
            share_server_address=None,
        )
        
        print(f"\n{'='*60}")
        print("âœ… éš§é“åˆ›å»ºæˆåŠŸï¼æ‚¨çš„å…¬å¼€è®¿é—®ä¿¡æ¯ï¼š")
        print(f"{'='*60}")
        
        # æ˜¾ç¤ºå­åŸŸåä¿¡æ¯
        if args.sd:
            print(f"ğŸ”§ ä½¿ç”¨å›ºå®šå­åŸŸå: {args.sd}")
            print(f"ğŸ’¡ æç¤ºï¼šä¸‹æ¬¡å¯ä½¿ç”¨ç›¸åŒå‘½ä»¤ä¿æŒåŸŸåä¸å˜")
        else:
            print(f"ğŸ”§ ä½¿ç”¨éšæœºä»¤ç‰Œ: {share_token[:16]}...")
            
        print(f"ğŸ“¢ å…¬å¼€ URL: {public_url}")
        print(f"ğŸ”— API æ ¹è·¯å¾„: {public_url}/")
        print(f"ğŸ‘¥ è¯´è¯äººåˆ—è¡¨: {public_url}/voice/speakers")
        print(f"ğŸ”„ æ¨¡å‹åˆ‡æ¢: {public_url}/set_model")
        print(f"{'='*60}")
        print(f"â³ æ­¤é“¾æ¥é»˜è®¤æœ‰æ•ˆæœŸä¸º 72 å°æ—¶ã€‚")
        print(f"{'='*60}\n")
        
    except requests.exceptions.ConnectionError:
        print(f"\nâš ï¸  ç½‘ç»œé”™è¯¯ï¼šæ— æ³•è¿æ¥åˆ° Gradio éš§é“æœåŠ¡å™¨ã€‚")
        print(f"   è¿™å¯èƒ½æ˜¯å› ä¸º Kaggle çš„ç½‘ç»œé™åˆ¶ã€‚")
        print(f"   æ‚¨ä»å¯é€šè¿‡ä»¥ä¸‹æœ¬åœ°åœ°å€è®¿é—® APIï¼š")
        print(f"   â€¢ http://localhost:{port}")
        print(f"   â€¢ http://{host}:{port}")
    except Exception as e:
        print(f"\nâŒ åˆ›å»ºéš§é“æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯ï¼š{type(e).__name__}: {e}")
        print(f"   å¤‡ç”¨æœ¬åœ°åœ°å€ï¼šhttp://localhost:{port}")
    
    # 3. ä¸»å¾ªç¯
    try:
        print("ğŸ› ï¸  æœåŠ¡è¿è¡Œä¸­ã€‚æŒ‰ Ctrl+C ç»ˆæ­¢ã€‚")
        while True:
            time.sleep(1)
    except KeyboardInterrupt:

        print("\nğŸ‘‹ æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­...")
